{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alancobu/UOC/blob/main/TyFdDd_PEC4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5CVvze_5lvz"
      },
      "source": [
        "# PEC 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2cCeZkHWHjx"
      },
      "source": [
        "# 1. DBPedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3meVeae3xxR"
      },
      "source": [
        "En este notebook seguiremos utilizando las librerías python para trabajar con tripletas:\n",
        "\n",
        "*   **`urllib`** para trabajar con URLs\n",
        "*   **`datetime`** para formato e interpretación de fechas\n",
        "*   **`rdflib`** para trabajar con tripletas RDF\n",
        "*   **`rdflib-jsonld`** para usar JSON-LD  \n",
        "*   **`SPARQLWrapper`** para ejecutar consultas SPARQL e importar los resultados en el notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF0eipz642YL",
        "outputId": "fc79ff96-cc9a-42b8-aace-e4eadaecc7ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\aland\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\aland\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n",
            "ERROR: Invalid requirement: '#instalar'\n",
            "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\aland\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rdflib\n",
        "!pip install -q rdflib-jsonld\n",
        "!pip install -q sparqlwrapper    #instalar SPARQLwrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2iJJfNz5Ej0",
        "outputId": "27934a41-af39-4afb-f76b-0b9bac33212a"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import urllib.request\n",
        "import rdflib\n",
        "import rdflib_jsonld\n",
        "from rdflib import Graph, plugin\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON, XML, N3, RDF , POST, GET, POSTDIRECTLY, CSV\n",
        "import warnings\n",
        "warnings.filterwarnings (\"ignore\")\n",
        "\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import json \n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unittest\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E_qUDBm46_K"
      },
      "source": [
        "## 1.1 Crear un wrapper para SPARQL \n",
        "\n",
        "\n",
        "Para posibilitar la navegación en los datos como un grafo y hacer consultas usando el lenguaje SPARQL se necesita un endpoint SPARQL que es una dirección web que responde a peticiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sH8Awat-5aEq"
      },
      "outputs": [],
      "source": [
        "# incluir funciones para tratar SPARQL\n",
        "\n",
        "def create_sparql_client ( endpoint , result_format=JSON , query_method=POST , token=None ):\n",
        "  ''' Crea un cliente SPARQL '''\n",
        "  sparql = SPARQLWrapper(endpoint) # instanciar \n",
        "  if token:\n",
        "    sparql.addCustomHttpHeader (\"Authorization\",\"Bearer {}\".format(token))\n",
        "  sparql.setMethod ( query_method )\n",
        "  sparql.setReturnFormat ( result_format )\n",
        "  if query_method == POST:\n",
        "    sparql.setRequestMethod(POSTDIRECTLY)\n",
        "\n",
        "  return sparql\n",
        "\n",
        "\n",
        "def query_sparql ( sparql , prefix, query ):\n",
        "  ''' Ejecuta una consulta SPARQL '''\n",
        "  sparql.setQuery (prefix+query)   # TODO: llamar a setQuery concatenando los prefijos y la consulta\n",
        "  results = sparql.query()               \n",
        "  if sparql.returnFormat == JSON:\n",
        "        return results._convertJSON()\n",
        "  return results.convert()\n",
        "\n",
        "\n",
        "def print_results ( results, limit =''):\n",
        "  ''' Imprime los resultados de una consulta SPARQL '''\n",
        "  resdata = results['results']['bindings']# TODO\n",
        "  if limit != '':\n",
        "      resdata = resdata = results['results']['bindings'][:limit] # TODO\n",
        "  for result in resdata:\n",
        "      for ans in result:\n",
        "          print('{0}: {1}'.format(ans, result[ans]['value']))\n",
        "      print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvX__pud5osa"
      },
      "source": [
        " La función `query_sparql` obtiene resultados de la ejecución de una consulta SPARQL con un conjunto definido sobre un endpoint.\n",
        "\n",
        " A continuación con las funciones anteriormente definidas vamos a ejecutar una consulta de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
        "    pass\n",
        "else:\n",
        "    # Handle target environment that doesn't support HTTPS verification\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI4Kzmjh50R9",
        "outputId": "7afaae61-3852-41a4-828a-0c04169b275c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "movie: http://dbpedia.org/resource/Aisa_Bhi_Hota_Hai\n",
            "title: Aisa Bhi Hota Hai\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aisa_Hota_Hai\n",
            "title: Aisa Hota Hai\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aisha_(upcoming_film)\n",
            "title: Aisha (upcoming film)\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aithe\n",
            "title: Aithe\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aiye\n",
            "title: Aiye\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# usar endpoint de la DBpedia\n",
        "dbpedia_endpoint = 'https://dbpedia.org/sparql'\n",
        "\n",
        "\n",
        "# crear cliente SPARQL \n",
        "sparql = create_sparql_client ( dbpedia_endpoint ,result_format=JSON , query_method=POST , token=None)  # TODO : usar la funcion adecuada para crear un cliente SPARQL \n",
        "\n",
        "# definir prefijos\n",
        "prefix = '''\n",
        "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX dbp: <http://dbpedia.org/property/>\n",
        "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "    PREFIX dct: <http://purl.org/dc/terms/>\n",
        "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
        "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbc: <http://dbpedia.org/resource/Category:>\n",
        "    PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
        "'''\n",
        "\n",
        "# definir consulta\n",
        "select_all_movies_query = \"\"\"\n",
        "    SELECT ?movie ?title\n",
        "    WHERE {\n",
        "       ?movie rdf:type dbo:Film;\n",
        "              rdfs:label ?title .\n",
        "\n",
        "       FILTER (langMatches(lang(?title), \"en\"))\n",
        "    }\n",
        "    LIMIT 5\n",
        "\"\"\"\n",
        "results = query_sparql ( sparql , prefix ,  select_all_movies_query ) # TODO: llamar a la funcion adecuada para ejecutar la consulta\n",
        "\n",
        "print_results(results,limit=5)\n",
        "\n",
        "  # TODO : imprimir resultados (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqw_8lcm8axE"
      },
      "source": [
        "## 1.2 Usando dataframes y otras funciones auxiliares\n",
        "\n",
        "\n",
        "El resultado obtenido está en formato JSON que es un formato de intercambio muy útil aunque para nuestras necesidades es más conveniente poder tratar los resultados como un **dataframe**. Para esto usaremos las siguientes funciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xa2PkH3S9miO"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://dbpedia.org/resource/Aisa_Bhi_Hota_Hai</td>\n",
              "      <td>Aisa Bhi Hota Hai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://dbpedia.org/resource/Aisa_Hota_Hai</td>\n",
              "      <td>Aisa Hota Hai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://dbpedia.org/resource/Aisha_(upcoming_film)</td>\n",
              "      <td>Aisha (upcoming film)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://dbpedia.org/resource/Aithe</td>\n",
              "      <td>Aithe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://dbpedia.org/resource/Aiye</td>\n",
              "      <td>Aiye</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               movie                  title\n",
              "0      http://dbpedia.org/resource/Aisa_Bhi_Hota_Hai      Aisa Bhi Hota Hai\n",
              "1          http://dbpedia.org/resource/Aisa_Hota_Hai          Aisa Hota Hai\n",
              "2  http://dbpedia.org/resource/Aisha_(upcoming_film)  Aisha (upcoming film)\n",
              "3                  http://dbpedia.org/resource/Aithe                  Aithe\n",
              "4                   http://dbpedia.org/resource/Aiye                   Aiye"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def json2dataframe (results):\n",
        "    ''' Genera un dataframe con los resultados de una consulta SPARQL  '''\n",
        "    data = []\n",
        "    for result in results['results']['bindings']:    # TODO: obtener resultados\n",
        "        tmp = {}\n",
        "        for el in result:\n",
        "            tmp[el] = result[el]['value']\n",
        "        data.append(tmp)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def dataframe_results(sparql, prefix, query ):\n",
        "    ''' Ejecuta consulta y genera el dataframe '''\n",
        "    return json2dataframe(query_sparql(sparql, prefix,  query))  # TODO : llamar a la funcion para obtener el dataframe\n",
        "\n",
        "\n",
        "df = dataframe_results(sparql, prefix, select_all_movies_query)\n",
        "df.head(5)\n",
        "\n",
        "#assert isinstance(df, pd.DataFrame) == True\n",
        "#assert len (df) == 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "281Bv4yOBUiQ"
      },
      "source": [
        "Como se puede observar, en los resultados aparecen URIs por lo que vamos a necesitamos unas funciones auxiliares que permitan convertir de URI a etiqueta y viceversa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z9AECFIUBUBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antonio Banderas\n",
            "Charlie Chaplin\n",
            "http://dbpedia.org/resource/Kevin_Bacon\n",
            "http://dbpedia.org/resource/2008_FIFA_Club_World_Cup_squads\n"
          ]
        }
      ],
      "source": [
        "# Obtener el nombre asociado a una URI (en un idioma)\n",
        "\n",
        "def get_label (uri, lang = 'en'):\n",
        "  prefix = '''\n",
        "      PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "      PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "      PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "  '''\n",
        "  query = '''\n",
        "      SELECT ?label \n",
        "      WHERE {\n",
        "        \n",
        "        <%s> rdfs:label ?label. # TODO \n",
        "        FILTER (lang(?label) = \"%s\") # TODO\n",
        "      }\n",
        "  ''' % (uri,lang)\n",
        "\n",
        "  df = dataframe_results(sparql, prefix, query)  # TODO \n",
        "  return df['label'][0]  # TODO\n",
        "\n",
        "# Obtener la URI de una entidad a partir del nombre\n",
        "def get_URI ( name , lang = 'en' ):\n",
        "  prefix = '''\n",
        "      PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "      PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "      PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "  '''\n",
        "\n",
        "  query = '''\n",
        "      SELECT ?uri\n",
        "      WHERE {\n",
        "        ?uri foaf:name \"%s\"@%s.\n",
        "      } \n",
        "               \n",
        "  '''%(name,lang)\n",
        "\n",
        "  df = dataframe_results(sparql, prefix, query)  # TODO\n",
        "  return df['uri'][0]  # TODO\n",
        "\n",
        "print ( get_label ( get_URI (\"Antonio Banderas\") ) )\n",
        "print ( get_label ('http://dbpedia.org/resource/Charlie_Chaplin'))\n",
        "print (get_URI (\"Kevin Bacon\"))\n",
        "print (get_URI (\"Cristiano Ronaldo\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yj4OIIawkcR"
      },
      "source": [
        "Es posible preguntar por cualquiera de las propiedades que se almacenan de cada recurso. En esta ocasión vamos a preguntar por el lugar de nacimiento (`birthPlace`) de una persona, concretando con el nombre de la ciudad (en un solo idioma)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "msnH3F4HxRQy"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def birthPlace (person):\n",
        "  person_uri = get_URI(person) #TODO\n",
        "\n",
        "\n",
        "  prefix = '''\n",
        "  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "  PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "  PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "  '''\n",
        "  query = '''\n",
        "    SELECT ?placeLabel\n",
        "     WHERE {\n",
        "       <%s> dbo:birthplace ?placeLabel\n",
        "        \n",
        "     \n",
        "\n",
        "        # FILTER (LANG(?placeLabel) = \"en\")    \n",
        "    }\n",
        "  '''%(person_uri) # TODO\n",
        "\n",
        "  df = dataframe_results(sparql, prefix, query)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "birthPlace('Cristiano Ronaldo')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEsnppbBtK8k"
      },
      "source": [
        "Usaremos esas funciones para obtener la filmografía de un actor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M6OWjI8D11KW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film</th>\n",
              "      <th>filmLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://dbpedia.org/resource/Enormous_Changes_a...</td>\n",
              "      <td>Enormous Changes at the Last Minute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://dbpedia.org/resource/Beauty_Shop</td>\n",
              "      <td>Beauty Shop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://dbpedia.org/resource/Pyrates</td>\n",
              "      <td>Pyrates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://dbpedia.org/resource/My_One_and_Only_(f...</td>\n",
              "      <td>My One and Only (film)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://dbpedia.org/resource/Rails_&amp;_Ties</td>\n",
              "      <td>Rails &amp; Ties</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                film  \\\n",
              "0  http://dbpedia.org/resource/Enormous_Changes_a...   \n",
              "1            http://dbpedia.org/resource/Beauty_Shop   \n",
              "2                http://dbpedia.org/resource/Pyrates   \n",
              "3  http://dbpedia.org/resource/My_One_and_Only_(f...   \n",
              "4           http://dbpedia.org/resource/Rails_&_Ties   \n",
              "\n",
              "                             filmLabel  \n",
              "0  Enormous Changes at the Last Minute  \n",
              "1                          Beauty Shop  \n",
              "2                              Pyrates  \n",
              "3               My One and Only (film)  \n",
              "4                         Rails & Ties  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_filmography ( actor ):\n",
        "  actor_uri = get_URI (actor) #TODO\n",
        "  prefix = '''\n",
        "  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "  PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "  PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "  '''\n",
        "  query = '''\n",
        "       SELECT ?film ?filmLabel\n",
        "    WHERE {\n",
        "       ?film rdf:type dbo:Film;\n",
        "              rdfs:label ?filmLabel;\n",
        "              dbp:starring <%s>.\n",
        "        \n",
        "\n",
        "       FILTER (LANG(?filmLabel) = \"en\")\n",
        "    }\n",
        "      '''%(actor_uri) # TODO\n",
        "\n",
        "  df = dataframe_results(sparql, prefix, query)\n",
        "\n",
        "  return df\n",
        "\n",
        "get_filmography('Kevin Bacon').head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hKQEp6z2V6e"
      },
      "source": [
        "La siguiente consulta trata de recuperar listas de athletas que tiene la DbPedia junto con algunos datos como el nombre, fecha de nacimiento, altura, un resumen de su carrera y el país al que se asocian (si está disponible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LSZeWuo92U7u"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player</th>\n",
              "      <th>name</th>\n",
              "      <th>birthDate</th>\n",
              "      <th>height</th>\n",
              "      <th>abs</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://dbpedia.org/resource/Alain_Nebie</td>\n",
              "      <td>Alain Nebie</td>\n",
              "      <td>1984-09-08</td>\n",
              "      <td>1.86</td>\n",
              "      <td>Alain Nebie (born 8 September 1984 in Burkina ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://dbpedia.org/resource/Hlynur_Bæringsson</td>\n",
              "      <td>Hlynur Elías Bæringsson</td>\n",
              "      <td>1982-07-06</td>\n",
              "      <td>2.0066</td>\n",
              "      <td>Hlynur Elías Bæringsson (born 6 July 1982) is ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://dbpedia.org/resource/Hollie_Avil</td>\n",
              "      <td>Hollie Avil</td>\n",
              "      <td>1990-04-12</td>\n",
              "      <td>1.76</td>\n",
              "      <td>Hollie May Avil (born 12 April 1990) is an Eng...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://dbpedia.org/resource/Horacio_Nava</td>\n",
              "      <td>Horacio Nava</td>\n",
              "      <td>1982-01-20</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Horacio Nava Meza (born January 20, 1982 in Ch...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://dbpedia.org/resource/Horst_Bulau</td>\n",
              "      <td>Horst Bulau</td>\n",
              "      <td>1962-08-14</td>\n",
              "      <td>1.78</td>\n",
              "      <td>Horst Hardy Bulau (born 14 August 1962) is a C...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          player                     name  \\\n",
              "0        http://dbpedia.org/resource/Alain_Nebie              Alain Nebie   \n",
              "1  http://dbpedia.org/resource/Hlynur_Bæringsson  Hlynur Elías Bæringsson   \n",
              "2        http://dbpedia.org/resource/Hollie_Avil              Hollie Avil   \n",
              "3       http://dbpedia.org/resource/Horacio_Nava             Horacio Nava   \n",
              "4        http://dbpedia.org/resource/Horst_Bulau              Horst Bulau   \n",
              "\n",
              "    birthDate  height                                                abs  \\\n",
              "0  1984-09-08    1.86  Alain Nebie (born 8 September 1984 in Burkina ...   \n",
              "1  1982-07-06  2.0066  Hlynur Elías Bæringsson (born 6 July 1982) is ...   \n",
              "2  1990-04-12    1.76  Hollie May Avil (born 12 April 1990) is an Eng...   \n",
              "3  1982-01-20     1.8  Horacio Nava Meza (born January 20, 1982 in Ch...   \n",
              "4  1962-08-14    1.78  Horst Hardy Bulau (born 14 August 1962) is a C...   \n",
              "\n",
              "  country  \n",
              "0     NaN  \n",
              "1     NaN  \n",
              "2     NaN  \n",
              "3     NaN  \n",
              "4     NaN  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prefix = '''\n",
        "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX dbp: <http://dbpedia.org/property/>\n",
        "'''\n",
        "\n",
        "def get_query(limit, offset):\n",
        "    return f'''\n",
        "    SELECT DISTINCT ?player ?name ?birthDate ?height ?abs ?country  #TODO\n",
        "    WHERE {{\n",
        "       ?player rdf:type dbo:Athlete;  # TODO\n",
        "                dbo:birthDate ?birthDate; # TODO\n",
        "                dbo:height ?height;    # TODO\n",
        "                foaf:name  ?name;    # TODO\n",
        "                dbo:abstract ?abs. # TODO\n",
        "    OPTIONAL{{\n",
        "       ?player dbo:country ?country.}}\n",
        "    FILTER(LANG(?abs) = \"en\").  # TODO\n",
        "    FILTER(LANG(?name) = \"en\").\n",
        "    \n",
        "    }}\n",
        "    LIMIT {limit} OFFSET {offset}'''\n",
        "\n",
        "\n",
        "athlete_df = dataframe_results(sparql, prefix , get_query (5000,0))\n",
        "athlete_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BDcoMAkHlW4"
      },
      "source": [
        "Eliminar `http://dbpedia.org/resource/` de las columnas que lo tengan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1XEmzOtL11HY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player</th>\n",
              "      <th>name</th>\n",
              "      <th>birthDate</th>\n",
              "      <th>height</th>\n",
              "      <th>abs</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alain_Nebie</td>\n",
              "      <td>Alain Nebie</td>\n",
              "      <td>1984-09-08</td>\n",
              "      <td>1.86</td>\n",
              "      <td>Alain Nebie (born 8 September 1984 in Burkina ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hlynur_Bæringsson</td>\n",
              "      <td>Hlynur Elías Bæringsson</td>\n",
              "      <td>1982-07-06</td>\n",
              "      <td>2.0066</td>\n",
              "      <td>Hlynur Elías Bæringsson (born 6 July 1982) is ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hollie_Avil</td>\n",
              "      <td>Hollie Avil</td>\n",
              "      <td>1990-04-12</td>\n",
              "      <td>1.76</td>\n",
              "      <td>Hollie May Avil (born 12 April 1990) is an Eng...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Horacio_Nava</td>\n",
              "      <td>Horacio Nava</td>\n",
              "      <td>1982-01-20</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Horacio Nava Meza (born January 20, 1982 in Ch...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Horst_Bulau</td>\n",
              "      <td>Horst Bulau</td>\n",
              "      <td>1962-08-14</td>\n",
              "      <td>1.78</td>\n",
              "      <td>Horst Hardy Bulau (born 14 August 1962) is a C...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              player                     name   birthDate  height  \\\n",
              "0        Alain_Nebie              Alain Nebie  1984-09-08    1.86   \n",
              "1  Hlynur_Bæringsson  Hlynur Elías Bæringsson  1982-07-06  2.0066   \n",
              "2        Hollie_Avil              Hollie Avil  1990-04-12    1.76   \n",
              "3       Horacio_Nava             Horacio Nava  1982-01-20     1.8   \n",
              "4        Horst_Bulau              Horst Bulau  1962-08-14    1.78   \n",
              "\n",
              "                                                 abs country  \n",
              "0  Alain Nebie (born 8 September 1984 in Burkina ...     NaN  \n",
              "1  Hlynur Elías Bæringsson (born 6 July 1982) is ...     NaN  \n",
              "2  Hollie May Avil (born 12 April 1990) is an Eng...     NaN  \n",
              "3  Horacio Nava Meza (born January 20, 1982 in Ch...     NaN  \n",
              "4  Horst Hardy Bulau (born 14 August 1962) is a C...     NaN  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "athlete_df['player'] = athlete_df['player'].str.replace('http://dbpedia.org/resource/', '')\n",
        "athlete_df['country'] = athlete_df['country'].str.replace('http://dbpedia.org/resource/', '')\n",
        "\n",
        "athlete_df.head(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4-xbvyTIama"
      },
      "source": [
        "Eliminar los duplicados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pMCzpyEE11EQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player</th>\n",
              "      <th>name</th>\n",
              "      <th>birthDate</th>\n",
              "      <th>height</th>\n",
              "      <th>abs</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alain_Nebie</td>\n",
              "      <td>Alain Nebie</td>\n",
              "      <td>1984-09-08</td>\n",
              "      <td>1.86</td>\n",
              "      <td>Alain Nebie (born 8 September 1984 in Burkina ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hlynur_Bæringsson</td>\n",
              "      <td>Hlynur Elías Bæringsson</td>\n",
              "      <td>1982-07-06</td>\n",
              "      <td>2.0066</td>\n",
              "      <td>Hlynur Elías Bæringsson (born 6 July 1982) is ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hollie_Avil</td>\n",
              "      <td>Hollie Avil</td>\n",
              "      <td>1990-04-12</td>\n",
              "      <td>1.76</td>\n",
              "      <td>Hollie May Avil (born 12 April 1990) is an Eng...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Horacio_Nava</td>\n",
              "      <td>Horacio Nava</td>\n",
              "      <td>1982-01-20</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Horacio Nava Meza (born January 20, 1982 in Ch...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Horst_Bulau</td>\n",
              "      <td>Horst Bulau</td>\n",
              "      <td>1962-08-14</td>\n",
              "      <td>1.78</td>\n",
              "      <td>Horst Hardy Bulau (born 14 August 1962) is a C...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              player                     name   birthDate  height  \\\n",
              "0        Alain_Nebie              Alain Nebie  1984-09-08    1.86   \n",
              "1  Hlynur_Bæringsson  Hlynur Elías Bæringsson  1982-07-06  2.0066   \n",
              "2        Hollie_Avil              Hollie Avil  1990-04-12    1.76   \n",
              "3       Horacio_Nava             Horacio Nava  1982-01-20     1.8   \n",
              "4        Horst_Bulau              Horst Bulau  1962-08-14    1.78   \n",
              "\n",
              "                                                 abs country  \n",
              "0  Alain Nebie (born 8 September 1984 in Burkina ...     NaN  \n",
              "1  Hlynur Elías Bæringsson (born 6 July 1982) is ...     NaN  \n",
              "2  Hollie May Avil (born 12 April 1990) is an Eng...     NaN  \n",
              "3  Horacio Nava Meza (born January 20, 1982 in Ch...     NaN  \n",
              "4  Horst Hardy Bulau (born 14 August 1962) is a C...     NaN  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(athlete_df.player.unique()) == len (athlete_df)\n",
        "athlete_df=athlete_df.drop_duplicates(subset=[\"player\"])\n",
        "# TODO\n",
        "\n",
        "athlete_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  1.86   ,   2.0066 ,   1.76   ,   1.8    ,   1.78   ,   1.7272 ,\n",
              "         1.79   ,   1.71   ,   1.95   ,   1.7    ,   1.73   ,   1.93   ,\n",
              "         1.82   ,   1.87   ,   1.84   ,   2.1082 ,   1.905  ,   1.75   ,\n",
              "         1.524  ,   1.85   ,   1.8542 ,   1.92   ,   1.77   ,   1.81   ,\n",
              "         1.91   ,   1.8288 ,   1.8796 ,   1.778  ,   1.7526 ,   1.9    ,\n",
              "         2.05105,   1.69   ,   1.651  ,   1.68   ,   1.72   ,   2.0193 ,\n",
              "         1.97485,   1.83   ,   1.7018 ,   2.0574 ,   1.89865,   1.88   ,\n",
              "         1.8034 ,   1.6764 ,   1.96   ,   1.74   ,   1.65   ,   1.9812 ,\n",
              "         1.56   ,   1.6    ,   1.63   ,   1.9304 ,   1.64   ,   1.67   ,\n",
              "         1.98   ,   1.9558 ,   1.61   ,   1.6002 ,   1.89   ,   1.66   ,\n",
              "         1.94   ,   1.6256 ,   1.57   ,   1.49   ,   1.62   ,   1.99   ,\n",
              "         1.5748 ,   2.032  ,   2.03   ,   2.     ,   2.02   ,   1.29   ,\n",
              "         2.13   ,   2.00025,   2.0828 ,   1.58   ,   0.     ,   2.01   ,\n",
              "         3.82   ,   1.97   ,   1.54   , 190.     ,   1.53   ,   1.625  ,\n",
              "         1.5494 ,   1.59   ,   1.52   ,   1.9685 ,   2.159  ,   2.1336 ,\n",
              "         1.48   ,   1.676  ,   1.5    ,   1.87325,   2.2098 ,   2.0701 ,\n",
              "         2.04   ,   1.789  ,   1.55   , 183.     ,   1.6891 ,   1.7399 ,\n",
              "         1.7145 ,   2.05   ,   1.8415 ,   1.7907 , 184.     ,   1.4    ])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO\n",
        "athlete_df[\"height\"]=pd.to_numeric(athlete_df[\"height\"])\n",
        "athlete_df['height'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tXkfcvHN_6s"
      },
      "source": [
        "Encontrar si hay valores 'extraños' como athletas de más de 2 metros y medio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player</th>\n",
              "      <th>name</th>\n",
              "      <th>birthDate</th>\n",
              "      <th>height</th>\n",
              "      <th>abs</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>Edison_Reshketa</td>\n",
              "      <td>Edison Reshketa</td>\n",
              "      <td>1999-06-02</td>\n",
              "      <td>3.82</td>\n",
              "      <td>Edison Reshketa (born 2 June 1999 in Shkoder) ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1734</th>\n",
              "      <td>Alida_Marcovici</td>\n",
              "      <td>Alida Carmen Cioroianu Marcovici</td>\n",
              "      <td>1973-03-20</td>\n",
              "      <td>190.00</td>\n",
              "      <td>Alida Carmen Cioroianu Marcovici (born 20 Marc...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3731</th>\n",
              "      <td>Gulouchen_Karimova</td>\n",
              "      <td>Gulouchen Karimova</td>\n",
              "      <td>1979-09-16</td>\n",
              "      <td>183.00</td>\n",
              "      <td>Gulouchen Karimova (born 16 September 1979) is...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4811</th>\n",
              "      <td>Jolanda_Elshof</td>\n",
              "      <td>Jolanda Elshof</td>\n",
              "      <td>1975-08-05</td>\n",
              "      <td>184.00</td>\n",
              "      <td>Jolanda Elshof (born 5 August 1975) is a retir...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  player                              name   birthDate  \\\n",
              "1495     Edison_Reshketa                   Edison Reshketa  1999-06-02   \n",
              "1734     Alida_Marcovici  Alida Carmen Cioroianu Marcovici  1973-03-20   \n",
              "3731  Gulouchen_Karimova                Gulouchen Karimova  1979-09-16   \n",
              "4811      Jolanda_Elshof                    Jolanda Elshof  1975-08-05   \n",
              "\n",
              "      height                                                abs country  \n",
              "1495    3.82  Edison Reshketa (born 2 June 1999 in Shkoder) ...     NaN  \n",
              "1734  190.00  Alida Carmen Cioroianu Marcovici (born 20 Marc...     NaN  \n",
              "3731  183.00  Gulouchen Karimova (born 16 September 1979) is...     NaN  \n",
              "4811  184.00  Jolanda Elshof (born 5 August 1975) is a retir...     NaN  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "athlete_df.loc[athlete_df['height']>2.5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNI1Wn180fH4"
      },
      "source": [
        "# WikiData (I)\n",
        "\n",
        "\n",
        "**DBpedia** y **Wikidata** son dos proyectos de datos enlazados relacionados aunque diferentes. DBpedia se centra en generar datos abiertos enlazados a partir de documentos de la Wikipedia mientras que Wikidata se centra en crear (meta)datos abiertos enlazados para completar los documentos de la Wikipedia.\n",
        "\n",
        "\n",
        "Ambos proyectos proporcionan acceso a sus respectivos datos a través de accesos (endpoint) SPARQL.\n",
        "\n",
        "Supongamos la siguiente consulta:\n",
        "```\n",
        "SELECT ?person\n",
        "WHERE {\n",
        "?person wdt:P106 wd:Q5482740 .\n",
        "}\n",
        "```\n",
        "\n",
        "Se define un **sujeto** de interés `?person` que será lo que aparezca como una columna de resultados. Las restricciones son que `wdt:P106` sea `wd:Q5482740` donde el prefijo `wdt:` significa que se va a especificar un atributo y `wd:`significa que se especifica un valor del atributo. `P106` establece una ocupación y `Q5482740` indica que es programador. Es decir, trata de localizar a las personas cuya ocupación es ser programador.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YgHeZTyr10gJ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.wikidata.org/entity/Q80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://www.wikidata.org/entity/Q360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://www.wikidata.org/entity/Q4864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://www.wikidata.org/entity/Q5284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://www.wikidata.org/entity/Q7259</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 person\n",
              "0    http://www.wikidata.org/entity/Q80\n",
              "1   http://www.wikidata.org/entity/Q360\n",
              "2  http://www.wikidata.org/entity/Q4864\n",
              "3  http://www.wikidata.org/entity/Q5284\n",
              "4  http://www.wikidata.org/entity/Q7259"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
        "\n",
        "wd_sparql = create_sparql_client ( wikidata_endpoint ,result_format=JSON , query_method=POST , token=None) # TODO : crear cliente sparql\n",
        "\n",
        "q = '''SELECT ?person\n",
        "        WHERE {\n",
        "                ?person wdt:P106 wd:Q5482740 .\n",
        "        }'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM3FuNxxWnrH"
      },
      "source": [
        "Si observamos los resultados obtenemos una serie de códigos de personas, concretamente `wd:Q80` es [Tim Berners-Lee](https://www.wikidata.org/wiki/Q80), el inventor de la web. \n",
        "\n",
        "\n",
        "Como estos códigos no son muy intuitivos, WikiData dispone de una servicio de etiquetado que ayuda a traducir el código en un nombre. Para obtener el nombre de `person` sólo hay que añadir el atributo `label`: `personlabel` y añadirlo a la parte `SELECT` de la consulta.\n",
        "\n",
        "También se puede añadir un filtro (`FILTER`) para obtener los resultados en un idioma concreto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ERoc5An_10Yw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person</th>\n",
              "      <th>personLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.wikidata.org/entity/Q80</td>\n",
              "      <td>Tim Berners-Lee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://www.wikidata.org/entity/Q360</td>\n",
              "      <td>Julian Assange</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://www.wikidata.org/entity/Q4864</td>\n",
              "      <td>Eugene Kaspersky</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 person       personLabel\n",
              "0    http://www.wikidata.org/entity/Q80   Tim Berners-Lee\n",
              "1   http://www.wikidata.org/entity/Q360    Julian Assange\n",
              "2  http://www.wikidata.org/entity/Q4864  Eugene Kaspersky"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q = '''SELECT  ?person ?personLabel\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740 .\n",
        "\n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en,es,ca\". }\n",
        "  \n",
        "}'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ado2M7XeYoxD"
      },
      "source": [
        "Tenemos los resultados pero se puede restringir más a aquellos que hayan tenido una aportación importante ( `wdt:P800`) a la industria del software:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_2f-D9Gc10VZ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person</th>\n",
              "      <th>workLabel</th>\n",
              "      <th>personLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.wikidata.org/entity/Q92768</td>\n",
              "      <td>PHP</td>\n",
              "      <td>Rasmus Lerdorf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://www.wikidata.org/entity/Q92768</td>\n",
              "      <td>PHP</td>\n",
              "      <td>Rasmus Lerdorf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://www.wikidata.org/entity/Q92768</td>\n",
              "      <td>PHP</td>\n",
              "      <td>Rasmus Lerdorf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://www.wikidata.org/entity/Q92768</td>\n",
              "      <td>PHP</td>\n",
              "      <td>Rasmus Lerdorf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://www.wikidata.org/entity/Q92768</td>\n",
              "      <td>بي إتش بي</td>\n",
              "      <td>Rasmus Lerdorf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10663</th>\n",
              "      <td>http://www.wikidata.org/entity/Q66095348</td>\n",
              "      <td>Working with MediaWiki</td>\n",
              "      <td>Yaron Koren</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10664</th>\n",
              "      <td>http://www.wikidata.org/entity/Q66095348</td>\n",
              "      <td>Working with MediaWiki</td>\n",
              "      <td>Yaron Koren</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10665</th>\n",
              "      <td>http://www.wikidata.org/entity/Q66095348</td>\n",
              "      <td>Working with MediaWiki</td>\n",
              "      <td>Yaron Koren</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10666</th>\n",
              "      <td>http://www.wikidata.org/entity/Q66095348</td>\n",
              "      <td>Working with MediaWiki</td>\n",
              "      <td>Yaron Koren</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10667</th>\n",
              "      <td>http://www.wikidata.org/entity/Q66095348</td>\n",
              "      <td>Working with MediaWiki</td>\n",
              "      <td>Yaron Koren</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10668 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         person               workLabel  \\\n",
              "0         http://www.wikidata.org/entity/Q92768                     PHP   \n",
              "1         http://www.wikidata.org/entity/Q92768                     PHP   \n",
              "2         http://www.wikidata.org/entity/Q92768                     PHP   \n",
              "3         http://www.wikidata.org/entity/Q92768                     PHP   \n",
              "4         http://www.wikidata.org/entity/Q92768               بي إتش بي   \n",
              "...                                         ...                     ...   \n",
              "10663  http://www.wikidata.org/entity/Q66095348  Working with MediaWiki   \n",
              "10664  http://www.wikidata.org/entity/Q66095348  Working with MediaWiki   \n",
              "10665  http://www.wikidata.org/entity/Q66095348  Working with MediaWiki   \n",
              "10666  http://www.wikidata.org/entity/Q66095348  Working with MediaWiki   \n",
              "10667  http://www.wikidata.org/entity/Q66095348  Working with MediaWiki   \n",
              "\n",
              "          personLabel  \n",
              "0      Rasmus Lerdorf  \n",
              "1      Rasmus Lerdorf  \n",
              "2      Rasmus Lerdorf  \n",
              "3      Rasmus Lerdorf  \n",
              "4      Rasmus Lerdorf  \n",
              "...               ...  \n",
              "10663     Yaron Koren  \n",
              "10664     Yaron Koren  \n",
              "10665     Yaron Koren  \n",
              "10666     Yaron Koren  \n",
              "10667     Yaron Koren  \n",
              "\n",
              "[10668 rows x 3 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel ?workLabel\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740 .\n",
        "  \n",
        "  ?person wdt:P800 ?work.\n",
        "\n",
        "  ?work rdfs:label ?workLabel.\n",
        "   # TODO : buscar el trabajo por el que son conocidos los programadores\n",
        "  \n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
        "}\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIJaoBuZt4y"
      },
      "source": [
        "Aunque se observa que aparecen muchos resultados replicados varias veces pues si alguien tiene varias aportaciones importantes aparecerá varias veces. Por lo tanto, hay que agrupar todos las aportaciones en un solo atributo.\n",
        "\n",
        "Para ello, las funciones de agrupamiento `GROUP BY` y `GROUP_CONCAT` pueden ser útiles. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BrlXfP1201HF"
      },
      "outputs": [
        {
          "ename": "QueryBadFormed",
          "evalue": "QueryBadFormed: a bad request has been sent to the endpoint, probably the sparql query is bad formed. \n\nResponse:\nb'SPARQL-QUERY: queryStr=\\nSELECT DISTINCT ?person ?personLabel \\n( ?works ) #TODO\\nWHERE {\\n  ?person wdt:P106 wd:Q5482740.\\n  ?person                     .  # TODO : obtener el nombre de la persona \\n  ?person                     .  # TODO\\n                   ?workLabel .  # TODO    \\n  \\n  FILTER (   )  # TODO: filtrar por 1 solo idioma\\n  FILTER (   )    # TODO: filtrar por 1 solo idioma\\n}\\n # TODO\\n\\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Encountered \" \")\" \") \"\" at line 3, column 10.\\nWas expecting one of:\\n    \"=\" ...\\n    \"!=\" ...\\n    \">\" ...\\n    \"<\" ...\\n    \"<=\" ...\\n    \">=\" ...\\n    \"||\" ...\\n    \"&&\" ...\\n    \"+\" ...\\n    \"-\" ...\\n    \"*\" ...\\n    \"/\" ...\\n    \"as\" ...\\n    \"in\" ...\\n    \"not in\" ...\\n    <INTEGER_POSITIVE> ...\\n    <INTEGER_NEGATIVE> ...\\n    <DECIMAL_POSITIVE> ...\\n    <DECIMAL_NEGATIVE> ...\\n    <DOUBLE_POSITIVE> ...\\n    <DOUBLE_NEGATIVE> ...\\n    \\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doPost(QueryServlet.java:275)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doPost(RESTServlet.java:269)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doPost(MultiTenancyServlet.java:195)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:320)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:82)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:49)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:108)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:59)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: org.openrdf.query.MalformedQueryException: Encountered \" \")\" \") \"\" at line 3, column 10.\\nWas expecting one of:\\n    \"=\" ...\\n    \"!=\" ...\\n    \">\" ...\\n    \"<\" ...\\n    \"<=\" ...\\n    \">=\" ...\\n    \"||\" ...\\n    \"&&\" ...\\n    \"+\" ...\\n    \"-\" ...\\n    \"*\" ...\\n    \"/\" ...\\n    \"as\" ...\\n    \"in\" ...\\n    \"not in\" ...\\n    <INTEGER_POSITIVE> ...\\n    <INTEGER_NEGATIVE> ...\\n    <DECIMAL_POSITIVE> ...\\n    <DECIMAL_NEGATIVE> ...\\n    <DOUBLE_POSITIVE> ...\\n    <DOUBLE_NEGATIVE> ...\\n    \\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:400)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.ParseException: Encountered \" \")\" \") \"\" at line 3, column 10.\\nWas expecting one of:\\n    \"=\" ...\\n    \"!=\" ...\\n    \">\" ...\\n    \"<\" ...\\n    \"<=\" ...\\n    \">=\" ...\\n    \"||\" ...\\n    \"&&\" ...\\n    \"+\" ...\\n    \"-\" ...\\n    \"*\" ...\\n    \"/\" ...\\n    \"as\" ...\\n    \"in\" ...\\n    \"not in\" ...\\n    <INTEGER_POSITIVE> ...\\n    <INTEGER_NEGATIVE> ...\\n    <DECIMAL_POSITIVE> ...\\n    <DECIMAL_NEGATIVE> ...\\n    <DOUBLE_POSITIVE> ...\\n    <DOUBLE_NEGATIVE> ...\\n    \\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.generateParseException(SyntaxTreeBuilder.java:9722)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_consume_token(SyntaxTreeBuilder.java:9589)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ProjectionElem(SyntaxTreeBuilder.java:542)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Select(SyntaxTreeBuilder.java:489)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:352)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py\u001b[0m in \u001b[0;36m_query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1072\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1074\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturnFormat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mQueryBadFormed\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-28-7f651c938e39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m '''\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe_results\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwd_sparql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-7-8f5fff9778f6>\u001b[0m in \u001b[0;36mdataframe_results\u001b[1;34m(sparql, prefix, query)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdataframe_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;34m''' Ejecuta consulta y genera el dataframe '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mjson2dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_sparql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# TODO : llamar a la funcion para obtener el dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-4-33b61081ba09>\u001b[0m in \u001b[0;36mquery_sparql\u001b[1;34m(sparql, prefix, query)\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[1;34m''' Ejecuta una consulta SPARQL '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m   \u001b[0msparql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetQuery\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# TODO: llamar a setQuery concatenando los prefijos y la consulta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m   \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msparql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturnFormat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mJSON\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convertJSON\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1105\u001b[0m             \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mQueryResult\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \"\"\"\n\u001b[1;32m-> 1107\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mQueryResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mqueryAndConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py\u001b[0m in \u001b[0;36m_query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1075\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1077\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryBadFormed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1078\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m404\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mEndPointNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mQueryBadFormed\u001b[0m: QueryBadFormed: a bad request has been sent to the endpoint, probably the sparql query is bad formed. \n\nResponse:\nb'SPARQL-QUERY: queryStr=\\nSELECT DISTINCT ?person ?personLabel \\n( ?works ) #TODO\\nWHERE {\\n  ?person wdt:P106 wd:Q5482740.\\n  ?person                     .  # TODO : obtener el nombre de la persona \\n  ?person                     .  # TODO\\n                   ?workLabel .  # TODO    \\n  \\n  FILTER (   )  # TODO: filtrar por 1 solo idioma\\n  FILTER (   )    # TODO: filtrar por 1 solo idioma\\n}\\n # TODO\\n\\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Encountered \" \")\" \") \"\" at line 3, column 10.\\nWas expecting one of:\\n    \"=\" ...\\n    \"!=\" ...\\n    \">\" ...\\n    \"<\" ...\\n    \"<=\" ...\\n    \">=\" ...\\n    \"||\" ...\\n    \"&&\" ...\\n    \"+\" ...\\n    \"-\" ...\\n    \"*\" ...\\n    \"/\" ...\\n    \"as\" ...\\n    \"in\" ...\\n    \"not in\" ...\\n    <INTEGER_POSITIVE> ...\\n    <INTEGER_NEGATIVE> ...\\n    <DECIMAL_POSITIVE> ...\\n    <DECIMAL_NEGATIVE> ...\\n    <DOUBLE_POSITIVE> ...\\n    <DOUBLE_NEGATIVE> ...\\n    \\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doPost(QueryServlet.java:275)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doPost(RESTServlet.java:269)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doPost(MultiTenancyServlet.java:195)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:320)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:82)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:49)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:108)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:59)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: org.openrdf.query.MalformedQueryException: Encountered \" \")\" \") \"\" at line 3, column 10.\\nWas expecting one of:\\n    \"=\" ...\\n    \"!=\" ...\\n    \">\" ...\\n    \"<\" ...\\n    \"<=\" ...\\n    \">=\" ...\\n    \"||\" ...\\n    \"&&\" ...\\n    \"+\" ...\\n    \"-\" ...\\n    \"*\" ...\\n    \"/\" ...\\n    \"as\" ...\\n    \"in\" ...\\n    \"not in\" ...\\n    <INTEGER_POSITIVE> ...\\n    <INTEGER_NEGATIVE> ...\\n    <DECIMAL_POSITIVE> ...\\n    <DECIMAL_NEGATIVE> ...\\n    <DOUBLE_POSITIVE> ...\\n    <DOUBLE_NEGATIVE> ...\\n    \\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:400)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.ParseException: Encountered \" \")\" \") \"\" at line 3, column 10.\\nWas expecting one of:\\n    \"=\" ...\\n    \"!=\" ...\\n    \">\" ...\\n    \"<\" ...\\n    \"<=\" ...\\n    \">=\" ...\\n    \"||\" ...\\n    \"&&\" ...\\n    \"+\" ...\\n    \"-\" ...\\n    \"*\" ...\\n    \"/\" ...\\n    \"as\" ...\\n    \"in\" ...\\n    \"not in\" ...\\n    <INTEGER_POSITIVE> ...\\n    <INTEGER_NEGATIVE> ...\\n    <DECIMAL_POSITIVE> ...\\n    <DECIMAL_NEGATIVE> ...\\n    <DOUBLE_POSITIVE> ...\\n    <DOUBLE_NEGATIVE> ...\\n    \\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.generateParseException(SyntaxTreeBuilder.java:9722)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_consume_token(SyntaxTreeBuilder.java:9589)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ProjectionElem(SyntaxTreeBuilder.java:542)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Select(SyntaxTreeBuilder.java:489)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:352)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'"
          ]
        }
      ],
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel \n",
        "( ?works ) #TODO\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740.\n",
        "  ?person                     .  # TODO : obtener el nombre de la persona \n",
        "  ?person                     .  # TODO\n",
        "                   ?workLabel .  # TODO    \n",
        "  \n",
        "  FILTER (   )  # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (   )    # TODO: filtrar por 1 solo idioma\n",
        "}\n",
        " # TODO\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eORhKIKja0ox"
      },
      "source": [
        "Ahora es el momento de que les pongamos cara. Es posible obtener las fotografías de algunas de estas personas con `wdt:P18` para obtener una URL con una foto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-kzQDBx01FX"
      },
      "outputs": [],
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel \n",
        "               ( ?works ) #TODO\n",
        "                ?image\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740.\n",
        "  ?person                     .  # TODO : obtener el nombre de la persona \n",
        "  ?person                     .  # TODO\n",
        "                   ?workLabel .  # TODO    \n",
        "\n",
        "                   ?image # TODO\n",
        "  \n",
        "  FILTER (  )  # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (  )    # TODO: filtrar por 1 solo idioma\n",
        "}\n",
        "# TODO\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVGMkb6bsBx"
      },
      "source": [
        "Pudiendo visualizarlas dentro en el Dataframe con la siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMrAHUjIAasG"
      },
      "outputs": [],
      "source": [
        "def image_formatter(im, width=\"80px\"):\n",
        "    return f'<img src=\"%s\" width =\"%s\">' % (im, width)\n",
        "\n",
        "\n",
        "HTML(df.dropna().head(5).to_html(formatters={'': image_formatter}, # TODO: seleccionar columna de la imagen\n",
        "                                 escape=False)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv1bt0i4Cizv"
      },
      "source": [
        "\n",
        "Por último, de dónde proceden estas personas. Cada persona puede tener un atributo `country` (`wdt:P19`) y éste a su vez unas coordenadas (`wdt:P625`) con las que conseguir una latitud y longitud. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkHn6C7A01CL"
      },
      "outputs": [],
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel ?image ?lat ?lon\n",
        "\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740.\n",
        "  ?person                     .  # TODO : obtener el nombre de la persona \n",
        "  ?person                     .  # TODO\n",
        "                   \n",
        "\n",
        "                        ?image # TODO\n",
        "\n",
        "    # TODO\n",
        "    \n",
        "  \n",
        "  FILTER (  )  # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (  )    # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (  ) \n",
        "  \n",
        "}\n",
        "# TODO\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZoCaKjhcyhJ"
      },
      "source": [
        "Y ponerlos en un mapa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DegvB4F01Ai"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "world_map = folium.Map(prefer_canvas=True)\n",
        "\n",
        "for p in range ( df.shape[0]):\n",
        "  lat =  # TODO\n",
        "  lon =  # TODO\n",
        "  name =  # TODO: nombre\n",
        "  folium.CircleMarker ( [lat , lon ], \n",
        "                       radius=1.5, \n",
        "                       line_color='#3186cc',\n",
        "                       fill_color='#3186cc', \n",
        "                       fill=True,\n",
        "                       tooltip = name\n",
        "                       ).add_to(world_map)\n",
        "\n",
        "world_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gW3CwU_pHwH"
      },
      "source": [
        "# WikiData (II)\n",
        "\n",
        "\n",
        "Ya hemos visto cómo se puede consultar WikiData y su sistema de identificadores. En esta ocasión hay que recuperar datos estadísticos de países de la Unión Europea. Para ello, hay que utilizar la propiedad `ser-miembro-de` (`member of`) (P463) con el objeto Unión Europea (Q458).\n",
        "\n",
        "Resulta sencillo usar el servicio de etiquetado `SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }`que permite obtener los nombres de las entidades de manera más cómoda.\n",
        "\n",
        "Además del nombre del país miembre de la UE vamos a recuperar algunos valores estadísticos: población (P1082) y superficie (P2046), si están disponibles.\n",
        "\n",
        "\n",
        "Nota: La consulta debería devolver `Kingdom of the Netherlands` con item Q29999 en la lista de países europeos en vez de `Netherlands` (Q55) que forma parte de del país pero no es un país. De forma similar a como Inglaterra forma parte del Reino Unido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ze81Xfm003Y"
      },
      "outputs": [],
      "source": [
        "q = '''\n",
        "SELECT \n",
        "  ?country ?countryLabel ?population ?area \n",
        "WHERE {\n",
        "  ?country  # TODO\n",
        "   ?population # TODO\n",
        "   ?area  # TODO\n",
        "\n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "}\n",
        "'''\n",
        "\n",
        "countries_df = dataframe_results (wd_sparql, '', q)\n",
        "countries_df.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abjp7vTkiDZ8"
      },
      "source": [
        "Analizar los tipos de datos que nos devuelve SPARQL y cómo se convierten en el DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLunphwtrqqp"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3G1ECHgiQLY"
      },
      "source": [
        "Convertir los tipos numéricos al formato más adecuado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4k1jeFirzsH"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "countries_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUdRhwzGq6qg"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "for i, label in enumerate(['population','area']):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    df_plot = countries_df[label].sort_values().dropna()\n",
        "    df_plot.plot(kind='barh', color='C0', ax=plt.gca());\n",
        "    plt.ylabel('')\n",
        "    plt.xticks(rotation=30)\n",
        "    plt.title(label.capitalize())\n",
        "    plt.ticklabel_format(style='plain', axis='x')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yD4Ll9ti73B"
      },
      "source": [
        "En esta ocasión la consulta es un poco más complicada. Se necesita obtener las capitales (P36) de las capitales de la UE y de la Asociación Europea de Libre Comercio (Q166546)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51U-NrnumUuP"
      },
      "outputs": [],
      "source": [
        "q = '''\n",
        "SELECT  \n",
        "  ?countryLabel ?capitalLabel ?population \n",
        "WHERE {\n",
        "  { # TODO\n",
        "  \n",
        "  \n",
        "  SERVICE wikibase:label { \n",
        "    bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". \n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "capital_df = dataframe_results (wd_sparql, '', q)\n",
        "capital_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZmH5je2nMQZ"
      },
      "source": [
        "Vamos a añadir obtener el nombre de los alcaldes actuales de esas capitales así como las coordenadas geográficas en las que se encuentran.\n",
        "\n",
        "Para obtener las coordenadas geográficas se necesitan añadir las siguientes líneas a la consulta:\n",
        "\n",
        "\n",
        "```\n",
        "?capital p:P625/psv:P625 ?node.\n",
        "?node wikibase:geoLatitude ?capital_lat.\n",
        "?node wikibase:geoLongitude ?capital_lon.\n",
        "```\n",
        "Mediante p:P625/psv:P625 se pasa al valor del nodo de las coordenadas de la localización (P625)y con `wikibase:geoLatitude` y  `wikibase:geoLongitude` se recuperan la latitud y la longitud.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEMZwkQ4t6-2"
      },
      "outputs": [],
      "source": [
        "q = '''\n",
        "SELECT  \n",
        "  ?countryLabel ?capitalLabel ?population \n",
        "  ?capital_lon ?capital_lat\n",
        "  ?mayorLabel \n",
        "WHERE {\n",
        "  # TODO\n",
        "  \n",
        "  \n",
        "  # Coordenadas\n",
        "  \n",
        "\n",
        "  SERVICE wikibase:label { \n",
        "    bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". \n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "capital_df = dataframe_results (wd_sparql, '', q)\n",
        "capital_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expTqbrIpZaC"
      },
      "source": [
        "Convertir los datos numéricos al formato adecuado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzcRUu1UuLjz"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "capital_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa5vOb8KqirD"
      },
      "source": [
        "Eliminar datos duplicados (si los hay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7dCuV91qtoD"
      },
      "outputs": [],
      "source": [
        "len(capital_df['capitalLabel'].unique()) == len (capital_df)\n",
        "\n",
        "# TODO\n",
        "\n",
        "capital_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg7Eb1sOuVIa"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "capital_df['population'].sort_values().plot(kind='barh', color='C0', title='Population')\n",
        "plt.ylabel('')\n",
        "plt.ticklabel_format(style='plain', axis='x')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQgwuaQPxHIZ"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "euro_map = folium.Map(location=[50,0],tiles=\"OpenStreetMap\", zoom_start=4)\n",
        "\n",
        "for p in range ( capital_df.shape[0]):\n",
        "  lat = # TODO\n",
        "  lon = # TODO\n",
        "  name = # TODO\n",
        "  folium.Marker ( [ , ], # TODO\n",
        "                       tooltip = name\n",
        "                       ).add_to(euro_map)\n",
        "\n",
        "euro_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z42J6JxYtXAj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0NNavx5gl8"
      },
      "source": [
        "# 4. Grafos : Conexiones aéreas\n",
        "\n",
        "En esta sección vamos a estudiar los grafos aplicados a las rutas aéreas. La idea es observar las rutas entre aeropuertos y encontrar vuelos entre ellos.\n",
        "\n",
        "Vamos a utilizar dos conjuntos de datos para el análisis:\n",
        "\n",
        "\n",
        "1.   **Aeropuertos**: Datos sobre los aeropuertos identificados por su código IATA\n",
        "2.   **Rutas**: Datos sobre rutas aéreas con origen y destino aeropuertos y vuelos entre ellos.\n",
        "\n",
        "Para obtener estos datos usaremos la URL `http://api.travelpayouts.com` que devuelve los datos en formato JSON. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdLLza7vxHCo"
      },
      "outputs": [],
      "source": [
        "def get_airport_data():\n",
        "    url = 'https://api.travelpayouts.com/data/en/airports.json'\n",
        "    with urllib.request.urlopen(url) as url:\n",
        "        airport_json = json.loads(url.read().decode(\"utf-8\"))\n",
        "    return airport_json\n",
        "\n",
        "def get_routes_data():\n",
        "    url = \"http://api.travelpayouts.com/data/routes.json\"\n",
        "    with urllib.request.urlopen(url) as url:\n",
        "        data = json.loads(url.read().decode(\"utf-8\"))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rgGoR73pwE"
      },
      "source": [
        "Los datos de los aeropuertos son el código IATA, nombre, ciudad, etc.\n",
        "\n",
        "Solamente se van a utilizar ciertos atributos de los aeropuertos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i0dorT-xG_j"
      },
      "outputs": [],
      "source": [
        "airport_json = # TODO\n",
        "\n",
        "columns = ['city_code', 'country_code','name','code']\n",
        "# TODO : convertir a dataframe\n",
        "airport_df = \n",
        "airport_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYpqDq9o5CSP"
      },
      "source": [
        "A continuación hay que filtrar los datos pues sólo vamos a analizar los aeropuertos de EE.UU. y de España:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMq-tcJTxG8b"
      },
      "outputs": [],
      "source": [
        "airport_us = # TODO\n",
        "\n",
        "airport_es = # TODO\n",
        "airport_es.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRw_61ay5lGJ"
      },
      "source": [
        "Se necesita unas variables que contengan la lista de códigos de los aeropuertos que usaremos más adelante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4VBVKi0xG6W"
      },
      "outputs": [],
      "source": [
        "airport_us_in =  # TODO\n",
        "airport_es_in =  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipEUezI354L9"
      },
      "source": [
        "Lo siguiente es cargar los datos de las rutas consistentes en códigos IATA de los aeropuertos, conexiones y detalles del vuelo, aunque sólo son de interés ciertas columnas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNHWj3I2xG4n"
      },
      "outputs": [],
      "source": [
        "routes_json =  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg3gl3QVxG0Y"
      },
      "outputs": [],
      "source": [
        "columns = ['departure_airport_iata', 'arrival_airport_iata']\n",
        "\n",
        "# TODO: convertir a dataframe\n",
        "routes_df = \n",
        "routes_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJAPzY-u-42U"
      },
      "source": [
        "Se necesita una columna más que cuente el número de vuelos entre dos aeropuertos. En el JSON de rutas hay un array con `planes` y lo que se necesita es el número de elementos de ese array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiPjyk72xGty"
      },
      "outputs": [],
      "source": [
        "routes_df['flights'] =  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQO8T0fdB2s-"
      },
      "source": [
        "Para realizar el análisis hay que filtrar las rutas con origen y destino EE.UU. y con origen y destino España."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re-HkMU8xGrr"
      },
      "outputs": [],
      "source": [
        "# TODO: Filtrar rutas con origen y destino USA\n",
        "routes_us_f = \n",
        "\n",
        "# TODO: Filtrar rutas con origen y destino España\n",
        "routes_es_f = \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiMmFNdmxGo4"
      },
      "outputs": [],
      "source": [
        "routes_es_f.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha0BMYWjCuFE"
      },
      "source": [
        "Hay que contar el número de rutas entre dos aeropuertos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuEwAkjnxGnO"
      },
      "outputs": [],
      "source": [
        "#TODO: Contar las rutas entre 2 aeropuertos\n",
        "\n",
        "routes_us_g = # TODO\n",
        "\n",
        "routes_es_g = # TODO\n",
        "\n",
        "routes_es_g.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUtNLWOQDPxa"
      },
      "source": [
        "Como se puede ver existen bastantes rutas así que hay que realizar un filtro de los aeropuertos que tengan más de 5 conexiones para el caso de EE.UU. y 3 conexiones para el caso de España."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vj6zLdoxGhL"
      },
      "outputs": [],
      "source": [
        "#TODO Filtrar las rutas que tengan más de 5 conexiones USA\n",
        "routes_us_g = \n",
        "#TODO Filtrar las rutas que tengan más de 3 conexiones ESP\n",
        "routes_es_g = \n",
        "routes_es_g.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGJYvdZeEQ6m"
      },
      "source": [
        "Ahora ya está todo listo para el grafo donde los **aeropuertos** son los **nodos** y las **rutas** entre ellos son los **arcos**.\n",
        "\n",
        "Usando los dataset que han sido preparados previamente, vamos a representar los grafos usando `networkx`y `matplotlib`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVdYXMibxGSi"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def draw_graph(data):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  g = nx.from_pandas_edgelist(data, \n",
        "                              source='departure_airport_iata', \n",
        "                              target='arrival_airport_iata')\n",
        "\n",
        "  layout = nx.spring_layout(g, iterations=50)\n",
        "\n",
        "  nx.draw_networkx_edges(g, layout, edge_color='#AAAAAA')\n",
        "\n",
        "  # TODO: obtener un array de nodos de aeropuertos de destino\n",
        "  dest = [ ] # node in g.nodes()\n",
        "\n",
        "  size = [g.degree(node) * 240 for node in g.nodes() if node in data.arrival_airport_iata.unique()]\n",
        "  nx.draw_networkx_nodes(g, layout, nodelist=dest, node_size=size, node_color='lightblue')\n",
        "\n",
        "  # TODO: obtener un array de nodos de aeropuertos de origen\n",
        "  orig = [ ] # node in g.nodes()\n",
        "\n",
        "  nx.draw_networkx_nodes(g, layout, nodelist=orig, node_size=300, node_color='#AAAAAA')\n",
        "  high_degree_orig = [node for node in g.nodes() if node in data.departure_airport_iata.unique() and g.degree(node) > 1]\n",
        "\n",
        "  nx.draw_networkx_nodes(g, layout, nodelist=high_degree_orig, node_size=300, node_color='#fc8d62')\n",
        "\n",
        "  orig_dict = dict(zip(orig, orig))\n",
        "  nx.draw_networkx_labels(g, layout, labels=orig_dict)\n",
        "  \n",
        "  plt.axis('off')\n",
        "  plt.title(\"Connections\")  \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "draw_graph(routes_es_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB13W1KtGIEP"
      },
      "source": [
        "Los nodos en azul son conexiones de origen y en naranja las conexiones de destino. El tamaño de los círculos indica el número de conexiones con origen y destino al nodo. Según el grafo anterior deberían observarse los principales aeropuertos españoles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNFzk93EFnx-"
      },
      "outputs": [],
      "source": [
        "draw_graph(routes_us_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKLp7NZiHBgX"
      },
      "source": [
        "En el caso de EE.UU. se pueden ver los principales aeropuertos: ORD, DEN y LAX. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ucAgFIPHcIO"
      },
      "source": [
        "A continuación vamos a definir diferentes métricas de centralidad que representan los factores de proximidad entre nodos en un grafo.\n",
        "\n",
        "La primera es el **grado de centralidad** que mide para cada nodo del grafo el número de conexiones con origen y destino a ese nodo:\n",
        "\n",
        "$ C_{D} (i) = \\sum_{ j=1 \\\\ (i \\neq j)}^{N}x_{ij} $\n",
        "\n",
        "\n",
        "La librería `networkx` dispone de una función para calcular el grado de centralidad de una grafo: `degree_centrality`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZJgRbm04X24"
      },
      "outputs": [],
      "source": [
        "def show_centrality(data):\n",
        "\n",
        "  g = nx.from_pandas_edgelist(data, \n",
        "                              source='departure_airport_iata', \n",
        "                              target='arrival_airport_iata')\n",
        "  # TODO: calcular \n",
        "  deg_cen = \n",
        "  data_deg_cen = pd.DataFrame(deg_cen.items())\n",
        "  data_deg_cen = data_deg_cen[data_deg_cen[1] > 0.05]\n",
        "  # plot the histogram \n",
        "  plt.barh(data_deg_cen[0], data_deg_cen[1])\n",
        "  plt.xlabel('Airports')\n",
        "  plt.ylabel('Degree Centrality')\n",
        "  plt.show()\n",
        "\n",
        "show_centrality(routes_es_g)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeVXVsJ7OCrc"
      },
      "outputs": [],
      "source": [
        "show_centrality(routes_us_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNDh-_XKLzWm"
      },
      "source": [
        "La otra métrica que vamos a usar es la **centralidad de intermediación** (betweenness centrality) que determina el número de rutas que pasan a través de un nodo en una red. Significa que puede haber un número de conexiones entre dos nodos a través de un nodo específico.\n",
        "\n",
        "$ C_{B} (i) = \\sum_{ j=1 \\\\ (i \\neq j)}^{N}  \\sum_{ k=1 \\\\ (k \\neq i)}^{j-1} \\frac {g_{jk}(i)}{g_{jk}} $\n",
        "\n",
        "También la librería `networkx` dispone de una función para calcular el grado de centralidad de una grafo: `betweenness_centrality`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRAfJS823PGw"
      },
      "outputs": [],
      "source": [
        "def show_bet_centrality(data):\n",
        "  g = nx.from_pandas_edgelist(data, source='departure_airport_iata', target='arrival_airport_iata')\n",
        "  bet_cen =  # TODO: calcular centralidad de intermediacion\n",
        "  data_bet_cen = pd.DataFrame(bet_cen.items())\n",
        "  # print(data)\n",
        "  data_bet_cen = data_bet_cen[data_bet_cen[1] > 0.05]\n",
        "  plt.bar(data_bet_cen[0], data_bet_cen[1])\n",
        "  plt.xlabel('Airports')\n",
        "  plt.ylabel('Betweenness Centrality')\n",
        "  plt.show()\n",
        "\n",
        "show_bet_centrality( routes_es_g )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AImzaEn8N1o1"
      },
      "outputs": [],
      "source": [
        "show_bet_centrality( routes_us_g )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRGJ5qpixF1B"
      },
      "source": [
        "Estos aeropuertos con más conexiones a través de ellos son los que más opciones ofrecen para las conexiones con otros destinos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhxfJ69UWLUw"
      },
      "source": [
        "## 4.1. Consulta de API: vuelos en tiempo real\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTYw73ExOlzX"
      },
      "source": [
        "En esta sección vamos a trabajar con datos sobre rutas aéreas, compañías aéreas y aeropuertos.\n",
        "\n",
        "Los datos en tiempo real los vamos obtener de la [API OpenSky](https://opensky-network.org/). Lo primero será cargar las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9COapkx5hYE"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/openskynetwork/opensky-api.git\n",
        "!pip install -e ./opensky-api/python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UevA0Vvg7PoQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./opensky-api/python')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPsZsBecPO5H"
      },
      "source": [
        "Con esta API vamos a obtener los datos de los vuelos que están en este instante sobrevolando la península ibérica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfd9pp7j6GgV"
      },
      "outputs": [],
      "source": [
        "from opensky_api import OpenSkyApi\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "api = OpenSkyApi()\n",
        "\n",
        "t = int(time.time())\n",
        "\n",
        "bboxSpain = [27.4335426 ,\t43.9933088 ,\t-18.3936845, \t4.5918885]\n",
        "\n",
        "states = api.get_states(time_secs= t, bbox=bboxSpain)\n",
        "for s in states.states[0:10]:\n",
        "  print(\"(%r, %r, %r, %r)\" % (s.longitude, s.latitude, s.velocity, s.callsign ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tIWdfolPjUa"
      },
      "source": [
        "Y esto lo vamos a representar en un mapa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE0CUpb4GPLN"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "world_map = folium.Map(location =[40.416775, -3.7388], tiles='stamenterrain', zoom_start=6,prefer_canvas=True)\n",
        "\n",
        "for s in states.states[0:100]:\n",
        "  try:\n",
        "    folium.Marker([ , ], # TODO: lat lon\n",
        "                tooltip=s.callsign + ' (' + s.origin_country + ')',\n",
        "                icon=folium.Icon(icon='plane', color='green')\n",
        "              ).add_to(world_map)\n",
        "  except:\n",
        "    pass\n",
        "world_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS7Q2_kD2dsG"
      },
      "source": [
        "# Parte 5. Grafos de conocimiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_NJmxE6XGoW"
      },
      "source": [
        "Un **[grafo de conocimiento](https://en.wikipedia.org/wiki/Knowledge_Graph)** consiste en una colección de información interrelacionada, normalmente limitada a un dominio específico y gestionado como un grafo. La unidad básica de un grafo de conocimiento es una tripleta *subjeto-predicado-objeto*, que en ocasiones se denota por (*head*, *relation*, *tail*) o más conciso (h, r, t).\n",
        "\n",
        "\n",
        "El grafo de conocimiento representa el conocimiento mediante *entidades* (como personas, localizaciones, organizaciones, incluso eventos) y sus relaciones. Cada tripleta define una conexión entre dos entidades en el grafo. Una ontología define las relaciones y entidades aceptables en el grafo.\n",
        "\n",
        "\n",
        "Veamos un ejemplo: Si un nodoA = `Putin` y el nodoB = `Rusia`, entonces es bastante probable que el arco que los une sea `presidenteDe`. Un nodo o entidad puede tener múltiples relaciones, así Putin no sólo es presidente de Rusia también trabajó para el servicio secreto de la Unión Soviética o KGB.\n",
        "\n",
        "La idea es la misma que la web semántica: hacer que toda la información en internet esté conectada y sea entendible por los ordenadores mediante estándares como `RDF` o `schema.org`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAxylTLNJ3-"
      },
      "source": [
        "Es posible construir un grafo de conocimiento a partir de texto, pero para ello hay que hacer que una máquina \"entienda\" el lenguaje natural. Esto se puede aproximar mediante las técnicas de **Procesamiento del Lenguaje Natural** (NLP por sus siglas en inglés) como segmentación de sentencias, análisis de dependencias, etiquetado del discurso y reconocimiento de entidades. Veremos una introdución a todo esto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Goos-ohrNZuy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlolUqqMVjV"
      },
      "source": [
        "## 5.1 Extracción de entidades\n",
        "\n",
        "El primer paso para construir un grafo de conocimiento es separar el texto de los documentos o artículos en sentencias u oraciones. De todas ellas nos vamos a centrar en aquellas que tienen exactamente 1 sujeto y 1 objeto.\n",
        "\n",
        "Cada sentencia se divide en tokens, cuando `spaCy` procesa el texto, añade un tag **`dep_`** a cada palabra indicando la función : sujeto, objeto, etc. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSlvOV5KMDlk"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"London is the capital and largest city of England and the United Kingdom.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, \"...\", token.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9DzMNO8Opjd"
      },
      "source": [
        "El sujeto (`nsubj`) en esta oración según el analizador de dependencias es `London`, aunque las entidades se pueden encontrar modificadas con `amond` o formar composiciones `compound`. El objeto de la sentencia es `England` (`pobj`) junto con `United Kingdom`. Por tanto, habría que extraer tanto el sujeto como el objeto junto con sus modificadores. Es conveniente ver la documentación de SpaCy para ver los diferentes tipos de tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dsy4Iyb91Xq"
      },
      "source": [
        "Existe sin embargo un API (Wikifier API) que permite etiquetar entidades en un texto. Como ejemplo de lo que puede hacer la API Wikifier, supongamos el siguiente texto sobre [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk) , extraído y modificado de la WikiPedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiondganZUyB"
      },
      "outputs": [],
      "source": [
        "EM_text = \"\"\"\n",
        "Elon Musk is a business magnate, industrial designer, and engineer. \n",
        "Elon Musk is the founder, CEO, CTO, and chief designer of SpaceX. \n",
        "Elon Musk is also early investor, CEO, and product architect of Tesla, Inc. \n",
        "Elon Musk is also the founder of The Boring Company and the co-founder of Neuralink. \n",
        "A centibillionaire, Musk became the richest person in the world in January 2021, with an estimated net worth of $185 billion at the time, surpassing Jeff Bezos. \n",
        "Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. \n",
        "Elon Musk briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. \n",
        "Elon Musk transferred to the University of Pennsylvania two years later, where Elon Musk received dual bachelor's degrees in economics and physics. \n",
        "Elon Musk moved to California in 1995 to attend Stanford University, but decided instead to pursue a business career. \n",
        "Elon Musk went on co-founding a web software company Zip2 with Elon Musk brother Kimbal Musk.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6IwYAfl-3II"
      },
      "source": [
        "Definimos una función que nos permita acceder a Wikifier API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFpkC_uf87Yy"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "from string import punctuation\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
        "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
        "\n",
        "\n",
        "def wikifier(text, lang=\"en\", threshold=0.8):\n",
        "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
        "    # URL.\n",
        "    data = urllib.parse.urlencode([\n",
        "        (\"text\", text), (\"lang\", lang),\n",
        "        (\"userKey\", \"icrogulbfeovyizvqttnfhjcixksdw\"),\n",
        "        (\"pageRankSqThreshold\", \"%g\" % threshold), \n",
        "        (\"applyPageRankSqThreshold\", \"true\"),\n",
        "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
        "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
        "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
        "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
        "    ])\n",
        "    url = \"http://www.wikifier.org/annotate-article\"\n",
        "    # CALL API.\n",
        "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
        "    with urllib.request.urlopen(req, timeout=60) as f:\n",
        "        response = f.read()\n",
        "        response = json.loads(response.decode(\"utf8\"))\n",
        "    # Output.\n",
        "    results = list()\n",
        "    for annotation in response[\"annotations\"]:\n",
        "        # Filter \n",
        "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
        "\n",
        "            # \n",
        "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
        "                label = 'Person'\n",
        "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
        "                label = 'Organization'\n",
        "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
        "                label = 'Location'\n",
        "            else:\n",
        "                label = None\n",
        "\n",
        "            results.append({'title': annotation['title'], \n",
        "                            'wikiId': annotation['wikiDataItemId'], \n",
        "                            'label': label,\n",
        "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdExAeDrAu3I"
      },
      "source": [
        "Wikifier API devuelve todas las clases a las que una entidad pertenece: busca en INSTANCE_OF y SUBCLASS_OF en toda la jerarquía. Por eso hay un filtro final que sólo busca las categorías de personal, organización o localización.\n",
        "\n",
        "\n",
        "Una cosa muy interesante de este API es que se obtienen los identificadores de  WikiData de las entidades."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaUgHUZh9NEI"
      },
      "outputs": [],
      "source": [
        "result = wikifier (EM_text)\n",
        "for entity in result:\n",
        "  print (\"%s - %s - (%s)\" % (entity['label'], entity['title'], entity['wikiId']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKcN5ZVZBRHB"
      },
      "source": [
        "La siguiente función que se encarga de obtener el listado de entidades y las relaciones que se establecen entre ellas dentro de una sentencia. Para ello hay que tratar sentencia por sentencia, eliminar los signos de puntuación y obtener las entidades con Wikifier API. \n",
        "\n",
        "Consideramos relaciones si dos entidades aparecen en la misma sentencia. Así se van uniendo por pares a una lista. Por último, se eliminan duplicados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdjogiZZCtEE"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    return '' # TODO\n",
        "\n",
        "def deduplicate_dict(d):\n",
        "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
        "\n",
        "\n",
        "def entities_extraction (text):\n",
        "  entities_list = list()\n",
        "  relations_list = list()\n",
        "\n",
        "  for sentence in  # TODO: recorrer sentencias\n",
        "    sentence =     # TODO: eliminar signos de puntuación\n",
        "\n",
        "    entities =     # TODO : llamar a wikifier\n",
        "    # TODO: añadir a la lista un diccionario por cada entidad con los valores de title, wikiId, label\n",
        "    entities_list.\n",
        "\n",
        "    for permutation in itertools.permutations(entities, 2): # generar relaciones \n",
        "      for source in permutation[0]['characters']:\n",
        "        for target in permutation[1]['characters']:\n",
        "          relations_list.append({'source': permutation[0]['title'],\n",
        "                                 'target': permutation[1]['title']})\n",
        "\n",
        "  return {'entities': [], 'relations': [] } # TODO : devolver un diccionario\n",
        "\n",
        "\n",
        "EM_dict = entities_extraction (EM_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92YeXqj8ESl3"
      },
      "outputs": [],
      "source": [
        "EM_dict['relations']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hejU4PaQutg1"
      },
      "source": [
        "Finalmente se contruye el grafo a partir de las entidades y las relaciones. Necesitamos expresar el grafo en forma de tabla para almacenar las tripletas, un campo con el origen, otro con el predicado y otro con el destino. Para ello, vamos a usar la estructura conocida que es el Dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne0mdk7vu-fR"
      },
      "outputs": [],
      "source": [
        "source = [] # TODO\n",
        "target = [] # TODO\n",
        "\n",
        "em_kg_df = pd.DataFrame({'source':source, 'target':target})\n",
        "em_kg_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3w_zdPp0LzE"
      },
      "source": [
        "Una vez están los datos en de Dataframe, podemos directamente volcarlo a un grafo usando la `networkx` que permite usar un dataframe como origen de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yUKCVFJwY6o"
      },
      "outputs": [],
      "source": [
        "G=nx.from_pandas_edgelist() # TODO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr8UWZcXwcSf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "TyFdDd_PEC4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
