{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TyFdDd_PEC4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alancobu/UOC/blob/main/TyFdDd_PEC4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5CVvze_5lvz"
      },
      "source": [
        "# PEC 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2cCeZkHWHjx"
      },
      "source": [
        "# 1. DBPedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3meVeae3xxR"
      },
      "source": [
        "En este notebook seguiremos utilizando las librerías python para trabajar con tripletas:\n",
        "\n",
        "*   **`urllib`** para trabajar con URLs\n",
        "*   **`datetime`** para formato e interpretación de fechas\n",
        "*   **`rdflib`** para trabajar con tripletas RDF\n",
        "*   **`rdflib-jsonld`** para usar JSON-LD  \n",
        "*   **`SPARQLWrapper`** para ejecutar consultas SPARQL e importar los resultados en el notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF0eipz642YL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc79ff96-cc9a-42b8-aace-e4eadaecc7ce"
      },
      "source": [
        "!pip install -q rdflib\n",
        "!pip install -q rdflib-jsonld\n",
        "!pip install -q sparqlwrapper    #instalar SPARQLwrapper"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 482 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 433 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2iJJfNz5Ej0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27934a41-af39-4afb-f76b-0b9bac33212a"
      },
      "source": [
        "import io\n",
        "import urllib.request\n",
        "import rdflib\n",
        "import rdflib_jsonld\n",
        "from rdflib import Graph, plugin\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON, XML, N3, RDF , POST, GET, POSTDIRECTLY, CSV\n",
        "import warnings\n",
        "warnings.filterwarnings (\"ignore\")\n",
        "\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import json \n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unittest\n",
        "from datetime import datetime"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rdflib_jsonld/__init__.py:12: DeprecationWarning: The rdflib-jsonld package has been integrated into rdflib as of rdflib==6.0.1.  Please remove rdflib-jsonld from your project's dependencies.\n",
            "  DeprecationWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E_qUDBm46_K"
      },
      "source": [
        "## 1.1 Crear un wrapper para SPARQL \n",
        "\n",
        "\n",
        "Para posibilitar la navegación en los datos como un grafo y hacer consultas usando el lenguaje SPARQL se necesita un endpoint SPARQL que es una dirección web que responde a peticiones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH8Awat-5aEq"
      },
      "source": [
        "# incluir funciones para tratar SPARQL\n",
        "\n",
        "def create_sparql_client ( endpoint , result_format=JSON , query_method=POST , token=None ):\n",
        "  ''' Crea un cliente SPARQL '''\n",
        "  sparql = SPARQLWrapper(endpoint) # instanciar \n",
        "  if token:\n",
        "    sparql.addCustomHttpHeader (\"Authorization\",\"Bearer {}\".format(token))\n",
        "  sparql.setMethod ( query_method )\n",
        "  sparql.setReturnFormat ( result_format )\n",
        "  if query_method == POST:\n",
        "    sparql.setRequestMethod(POSTDIRECTLY)\n",
        "\n",
        "  return sparql\n",
        "\n",
        "\n",
        "def query_sparql ( sparql , prefix, query ):\n",
        "  ''' Ejecuta una consulta SPARQL '''\n",
        "  sparql.setQuery (prefix+query)   # TODO: llamar a setQuery concatenando los prefijos y la consulta\n",
        "  results = sparql.query()               \n",
        "  if sparql.returnFormat == JSON:\n",
        "        return results._convertJSON()\n",
        "  return results.convert()\n",
        "\n",
        "\n",
        "def print_results ( results, limit =''):\n",
        "  ''' Imprime los resultados de una consulta SPARQL '''\n",
        "  resdata = results['results']['bindings']# TODO\n",
        "  if limit != '':\n",
        "      resdata = resdata = results['results']['bindings'][:limit] # TODO\n",
        "  for result in resdata:\n",
        "      for ans in result:\n",
        "          print('{0}: {1}'.format(ans, result[ans]['value']))\n",
        "      print()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvX__pud5osa"
      },
      "source": [
        " La función `query_sparql` obtiene resultados de la ejecución de una consulta SPARQL con un conjunto definido sobre un endpoint.\n",
        "\n",
        " A continuación con las funciones anteriormente definidas vamos a ejecutar una consulta de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI4Kzmjh50R9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afaae61-3852-41a4-828a-0c04169b275c"
      },
      "source": [
        "# usar endpoint de la DBpedia\n",
        "dbpedia_endpoint = 'https://dbpedia.org/sparql'\n",
        "\n",
        "\n",
        "# crear cliente SPARQL \n",
        "sparql = create_sparql_client ( dbpedia_endpoint ,result_format=JSON , query_method=POST , token=None)  # TODO : usar la funcion adecuada para crear un cliente SPARQL \n",
        "\n",
        "# definir prefijos\n",
        "prefix = '''\n",
        "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX dbp: <http://dbpedia.org/property/>\n",
        "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "    PREFIX dct: <http://purl.org/dc/terms/>\n",
        "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
        "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbc: <http://dbpedia.org/resource/Category:>\n",
        "    PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
        "'''\n",
        "\n",
        "# definir consulta\n",
        "select_all_movies_query = \"\"\"\n",
        "    SELECT ?movie ?title\n",
        "    WHERE {\n",
        "       ?movie rdf:type dbo:Film;\n",
        "              rdfs:label ?title .\n",
        "\n",
        "       FILTER (langMatches(lang(?title), \"en\"))\n",
        "    }\n",
        "    LIMIT 5\n",
        "\"\"\"\n",
        "results = query_sparql ( sparql , prefix ,  select_all_movies_query ) # TODO: llamar a la funcion adecuada para ejecutar la consulta\n",
        "\n",
        "print_results(results,limit=5)\n",
        "\n",
        "  # TODO : imprimir resultados (5)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movie: http://dbpedia.org/resource/Aisa_Bhi_Hota_Hai\n",
            "title: Aisa Bhi Hota Hai\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aisa_Hota_Hai\n",
            "title: Aisa Hota Hai\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aisha_(upcoming_film)\n",
            "title: Aisha (upcoming film)\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aithe\n",
            "title: Aithe\n",
            "\n",
            "movie: http://dbpedia.org/resource/Aiye\n",
            "title: Aiye\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqw_8lcm8axE"
      },
      "source": [
        "## 1.2 Usando dataframes y otras funciones auxiliares\n",
        "\n",
        "\n",
        "El resultado obtenido está en formato JSON que es un formato de intercambio muy útil aunque para nuestras necesidades es más conveniente poder tratar los resultados como un **dataframe**. Para esto usaremos las siguientes funciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa2PkH3S9miO"
      },
      "source": [
        "def json2dataframe (results):\n",
        "    ''' Genera un dataframe con los resultados de una consulta SPARQL  '''\n",
        "    data = []\n",
        "    for result in results[]:    # TODO: obtener resultados \n",
        "        tmp = {}\n",
        "        for el in result:\n",
        "            tmp[el] = result[el]['value']\n",
        "        data.append(tmp)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def dataframe_results(sparql, prefix, query ):\n",
        "    ''' Ejecuta consulta y genera el dataframe '''\n",
        "    return json2dataframe ()  # TODO : llamar a la funcion para obtener el dataframe \n",
        "\n",
        "\n",
        "df = dataframe_results(sparql, prefix, select_all_movies_query)\n",
        "df.head(5)\n",
        "\n",
        "#assert isinstance(df, pd.DataFrame) == True\n",
        "#assert len (df) == 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "281Bv4yOBUiQ"
      },
      "source": [
        "Como se puede observar, en los resultados aparecen URIs por lo que vamos a necesitamos unas funciones auxiliares que permitan convertir de URI a etiqueta y viceversa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9AECFIUBUBD"
      },
      "source": [
        "# Obtener el nombre asociado a una URI (en un idioma)\n",
        "\n",
        "def get_label (uri, lang = 'en'):\n",
        "  prefix = '''\n",
        "      PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "      PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "      PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "  '''\n",
        "  query = '''\n",
        "      SELECT ?label \n",
        "      WHERE {\n",
        "        <%s>  . # TODO \n",
        "\n",
        "        FILTER () # TODO\n",
        "      }\n",
        "  ''' % (uri,lang)\n",
        "\n",
        "  df = dataframe_results()  # TODO \n",
        "  return # TODO \n",
        "\n",
        "# Obtener la URI de una entidad a partir del nombre\n",
        "def get_URI ( name , lang = 'en' ):\n",
        "  prefix = '''\n",
        "      PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "      PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "      PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "  '''\n",
        "\n",
        "  query = '''\n",
        "      SELECT ?uri \n",
        "      WHERE {\n",
        "        ?uri ; # TODO: obtener la URI a partir de una etiqueta \n",
        "            a .       # TODO \n",
        "      }\n",
        "  ''' % (name, lang)\n",
        "\n",
        "  df = dataframe_results()  # TODO \n",
        "  return  # TODO\n",
        "\n",
        "print ( get_label ( get_URI (\"Antonio Banderas\") ) )\n",
        "print ( get_label ('http://dbpedia.org/resource/Charlie_Chaplin'))\n",
        "print (get_URI (\"Kevin Bacon\"))\n",
        "print (get_URI (\"Cristiano Ronaldo\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yj4OIIawkcR"
      },
      "source": [
        "Es posible preguntar por cualquiera de las propiedades que se almacenan de cada recurso. En esta ocasión vamos a preguntar por el lugar de nacimiento (`birthPlace`) de una persona, concretando con el nombre de la ciudad (en un solo idioma)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msnH3F4HxRQy"
      },
      "source": [
        "def birthPlace (person):\n",
        "  person_uri =  #TODO\n",
        "\n",
        "  query = '''\n",
        "    SELECT ?place ?placeLabel\n",
        "     WHERE {\n",
        "         # TODO\n",
        "\n",
        "         # TODO\n",
        "\n",
        "        FILTER (LANG(?placeLabel) = \"en\")    \n",
        "    }\n",
        "  ''' % ()  # TODO\n",
        "\n",
        "  df = dataframe_results(sparql, prefix, query)\n",
        "\n",
        "  return df\n",
        "\n",
        "birthPlace ('Cristiano Ronaldo').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEsnppbBtK8k"
      },
      "source": [
        "Usaremos esas funciones para obtener la filmografía de un actor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6OWjI8D11KW"
      },
      "source": [
        "def get_filmography ( actor ):\n",
        "  actor_uri = get_URI (actor) #TODO\n",
        "\n",
        "  query = '''\n",
        "    SELECT ?film ?filmLabel\n",
        "     WHERE {\n",
        "               # TODO : peliculas protagonizadas \n",
        "               \n",
        "        FILTER (LANG(?filmLabel) = \"en\")\n",
        "    }\n",
        "  ''' % ()  # TODO\n",
        "\n",
        "  df = dataframe_results(sparql, prefix, query)\n",
        "\n",
        "  return df\n",
        "\n",
        "get_filmography('Kevin Bacon').head(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hKQEp6z2V6e"
      },
      "source": [
        "La siguiente consulta trata de recuperar listas de athletas que tiene la DbPedia junto con algunos datos como el nombre, fecha de nacimiento, altura, un resumen de su carrera y el país al que se asocian (si está disponible)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZeWuo92U7u"
      },
      "source": [
        "prefix = '''\n",
        "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX dbp: <http://dbpedia.org/property/>\n",
        "'''\n",
        "\n",
        "def get_query(limit, offset):\n",
        "    return f'''\n",
        "    SELECT DISTINCT   #TODO\n",
        "    WHERE {{\n",
        "       ?player ;  # TODO\n",
        "       dbo:birthDate ; # TODO\n",
        "       dbo:height ;    # TODO\n",
        "       foaf:name  ;    # TODO\n",
        "       dbo:abstract  . # TODO\n",
        "\n",
        "       dbo:country .   # TODO\n",
        "    FILTER(LANG() = \"en\").  # TODO\n",
        "    FILTER(LANG() = \"en\").  # TODO\n",
        "    }}\n",
        "    LIMIT {limit} OFFSET {offset}'''\n",
        "\n",
        "\n",
        "athlete_df = dataframe_results(sparql, prefix , get_query (5000,0))\n",
        "athlete_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BDcoMAkHlW4"
      },
      "source": [
        "Eliminar `http://dbpedia.org/resource/` de las columnas que lo tengan:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XEmzOtL11HY"
      },
      "source": [
        "# TODO\n",
        "\n",
        "athlete_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4-xbvyTIama"
      },
      "source": [
        "Eliminar los duplicados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMCzpyEE11EQ"
      },
      "source": [
        "len(athlete_df.player.unique()) == len (athlete_df)\n",
        "\n",
        "# TODO\n",
        "\n",
        "athlete_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tXkfcvHN_6s"
      },
      "source": [
        "Encontrar si hay valores 'extraños' como athletas de más de 2 metros y medio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IotKDvqF11Aq"
      },
      "source": [
        "# TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNI1Wn180fH4"
      },
      "source": [
        "# WikiData (I)\n",
        "\n",
        "\n",
        "**DBpedia** y **Wikidata** son dos proyectos de datos enlazados relacionados aunque diferentes. DBpedia se centra en generar datos abiertos enlazados a partir de documentos de la Wikipedia mientras que Wikidata se centra en crear (meta)datos abiertos enlazados para completar los documentos de la Wikipedia.\n",
        "\n",
        "\n",
        "Ambos proyectos proporcionan acceso a sus respectivos datos a través de accesos (endpoint) SPARQL.\n",
        "\n",
        "Supongamos la siguiente consulta:\n",
        "```\n",
        "SELECT ?person\n",
        "WHERE {\n",
        "?person wdt:P106 wd:Q5482740 .\n",
        "}\n",
        "```\n",
        "\n",
        "Se define un **sujeto** de interés `?person` que será lo que aparezca como una columna de resultados. Las restricciones son que `wdt:P106` sea `wd:Q5482740` donde el prefijo `wdt:` significa que se va a especificar un atributo y `wd:`significa que se especifica un valor del atributo. `P106` establece una ocupación y `Q5482740` indica que es programador. Es decir, trata de localizar a las personas cuya ocupación es ser programador.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgHeZTyr10gJ"
      },
      "source": [
        "wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
        "\n",
        "wd_sparql = create_sparql_client (  )  # TODO : crear cliente sparql\n",
        "\n",
        "q = '''SELECT ?person\n",
        "        WHERE {\n",
        "                ?person wdt:P106 wd:Q5482740 .\n",
        "        }'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM3FuNxxWnrH"
      },
      "source": [
        "Si observamos los resultados obtenemos una serie de códigos de personas, concretamente `wd:Q80` es [Tim Berners-Lee](https://www.wikidata.org/wiki/Q80), el inventor de la web. \n",
        "\n",
        "\n",
        "Como estos códigos no son muy intuitivos, WikiData dispone de una servicio de etiquetado que ayuda a traducir el código en un nombre. Para obtener el nombre de `person` sólo hay que añadir el atributo `label`: `personlabel` y añadirlo a la parte `SELECT` de la consulta.\n",
        "\n",
        "También se puede añadir un filtro (`FILTER`) para obtener los resultados en un idioma concreto.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERoc5An_10Yw"
      },
      "source": [
        "q = '''SELECT  ?person ?personLabel\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740 .\n",
        "\n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en,es,ca\". }\n",
        "  \n",
        "}'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ado2M7XeYoxD"
      },
      "source": [
        "Tenemos los resultados pero se puede restringir más a aquellos que hayan tenido una aportación importante ( `wdt:P800`) a la industria del software:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2f-D9Gc10VZ"
      },
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel ?workLabel\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740 .\n",
        "  \n",
        "  ?person  # TODO : buscar el trabajo por el que son conocidos los programadores\n",
        "  \n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
        "}\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIJaoBuZt4y"
      },
      "source": [
        "Aunque se observa que aparecen muchos resultados replicados varias veces pues si alguien tiene varias aportaciones importantes aparecerá varias veces. Por lo tanto, hay que agrupar todos las aportaciones en un solo atributo.\n",
        "\n",
        "Para ello, las funciones de agrupamiento `GROUP BY` y `GROUP_CONCAT` pueden ser útiles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrlXfP1201HF"
      },
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel \n",
        "( ?works ) #TODO\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740.\n",
        "  ?person                     .  # TODO : obtener el nombre de la persona \n",
        "  ?person                     .  # TODO\n",
        "                   ?workLabel .  # TODO    \n",
        "  \n",
        "  FILTER (   )  # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (   )    # TODO: filtrar por 1 solo idioma\n",
        "}\n",
        " # TODO\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eORhKIKja0ox"
      },
      "source": [
        "Ahora es el momento de que les pongamos cara. Es posible obtener las fotografías de algunas de estas personas con `wdt:P18` para obtener una URL con una foto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-kzQDBx01FX"
      },
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel \n",
        "               ( ?works ) #TODO\n",
        "                ?image\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740.\n",
        "  ?person                     .  # TODO : obtener el nombre de la persona \n",
        "  ?person                     .  # TODO\n",
        "                   ?workLabel .  # TODO    \n",
        "\n",
        "                   ?image # TODO\n",
        "  \n",
        "  FILTER (  )  # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (  )    # TODO: filtrar por 1 solo idioma\n",
        "}\n",
        "# TODO\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVGMkb6bsBx"
      },
      "source": [
        "Pudiendo visualizarlas dentro en el Dataframe con la siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMrAHUjIAasG"
      },
      "source": [
        "def image_formatter(im, width=\"80px\"):\n",
        "    return f'<img src=\"%s\" width =\"%s\">' % (im, width)\n",
        "\n",
        "\n",
        "HTML(df.dropna().head(5).to_html(formatters={'': image_formatter}, # TODO: seleccionar columna de la imagen\n",
        "                                 escape=False)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv1bt0i4Cizv"
      },
      "source": [
        "\n",
        "Por último, de dónde proceden estas personas. Cada persona puede tener un atributo `country` (`wdt:P19`) y éste a su vez unas coordenadas (`wdt:P625`) con las que conseguir una latitud y longitud. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkHn6C7A01CL"
      },
      "source": [
        "q = '''\n",
        "SELECT DISTINCT ?person ?personLabel ?image ?lat ?lon\n",
        "\n",
        "WHERE {\n",
        "  ?person wdt:P106 wd:Q5482740.\n",
        "  ?person                     .  # TODO : obtener el nombre de la persona \n",
        "  ?person                     .  # TODO\n",
        "                   \n",
        "\n",
        "                        ?image # TODO\n",
        "\n",
        "    # TODO\n",
        "    \n",
        "  \n",
        "  FILTER (  )  # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (  )    # TODO: filtrar por 1 solo idioma\n",
        "  FILTER (  ) \n",
        "  \n",
        "}\n",
        "# TODO\n",
        "'''\n",
        "\n",
        "df = dataframe_results (wd_sparql, '', q)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZoCaKjhcyhJ"
      },
      "source": [
        "Y ponerlos en un mapa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DegvB4F01Ai"
      },
      "source": [
        "import folium\n",
        "\n",
        "world_map = folium.Map(prefer_canvas=True)\n",
        "\n",
        "for p in range ( df.shape[0]):\n",
        "  lat =  # TODO\n",
        "  lon =  # TODO\n",
        "  name =  # TODO: nombre\n",
        "  folium.CircleMarker ( [lat , lon ], \n",
        "                       radius=1.5, \n",
        "                       line_color='#3186cc',\n",
        "                       fill_color='#3186cc', \n",
        "                       fill=True,\n",
        "                       tooltip = name\n",
        "                       ).add_to(world_map)\n",
        "\n",
        "world_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gW3CwU_pHwH"
      },
      "source": [
        "# WikiData (II)\n",
        "\n",
        "\n",
        "Ya hemos visto cómo se puede consultar WikiData y su sistema de identificadores. En esta ocasión hay que recuperar datos estadísticos de países de la Unión Europea. Para ello, hay que utilizar la propiedad `ser-miembro-de` (`member of`) (P463) con el objeto Unión Europea (Q458).\n",
        "\n",
        "Resulta sencillo usar el servicio de etiquetado `SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }`que permite obtener los nombres de las entidades de manera más cómoda.\n",
        "\n",
        "Además del nombre del país miembre de la UE vamos a recuperar algunos valores estadísticos: población (P1082) y superficie (P2046), si están disponibles.\n",
        "\n",
        "\n",
        "Nota: La consulta debería devolver `Kingdom of the Netherlands` con item Q29999 en la lista de países europeos en vez de `Netherlands` (Q55) que forma parte de del país pero no es un país. De forma similar a como Inglaterra forma parte del Reino Unido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ze81Xfm003Y"
      },
      "source": [
        "q = '''\n",
        "SELECT \n",
        "  ?country ?countryLabel ?population ?area \n",
        "WHERE {\n",
        "  ?country  # TODO\n",
        "   ?population # TODO\n",
        "   ?area  # TODO\n",
        "\n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "}\n",
        "'''\n",
        "\n",
        "countries_df = dataframe_results (wd_sparql, '', q)\n",
        "countries_df.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abjp7vTkiDZ8"
      },
      "source": [
        "Analizar los tipos de datos que nos devuelve SPARQL y cómo se convierten en el DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLunphwtrqqp"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3G1ECHgiQLY"
      },
      "source": [
        "Convertir los tipos numéricos al formato más adecuado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4k1jeFirzsH"
      },
      "source": [
        "# TODO\n",
        "\n",
        "countries_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUdRhwzGq6qg"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "for i, label in enumerate(['population','area']):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    df_plot = countries_df[label].sort_values().dropna()\n",
        "    df_plot.plot(kind='barh', color='C0', ax=plt.gca());\n",
        "    plt.ylabel('')\n",
        "    plt.xticks(rotation=30)\n",
        "    plt.title(label.capitalize())\n",
        "    plt.ticklabel_format(style='plain', axis='x')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yD4Ll9ti73B"
      },
      "source": [
        "En esta ocasión la consulta es un poco más complicada. Se necesita obtener las capitales (P36) de las capitales de la UE y de la Asociación Europea de Libre Comercio (Q166546)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51U-NrnumUuP"
      },
      "source": [
        "q = '''\n",
        "SELECT  \n",
        "  ?countryLabel ?capitalLabel ?population \n",
        "WHERE {\n",
        "  { # TODO\n",
        "  \n",
        "  \n",
        "  SERVICE wikibase:label { \n",
        "    bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". \n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "capital_df = dataframe_results (wd_sparql, '', q)\n",
        "capital_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZmH5je2nMQZ"
      },
      "source": [
        "Vamos a añadir obtener el nombre de los alcaldes actuales de esas capitales así como las coordenadas geográficas en las que se encuentran.\n",
        "\n",
        "Para obtener las coordenadas geográficas se necesitan añadir las siguientes líneas a la consulta:\n",
        "\n",
        "\n",
        "```\n",
        "?capital p:P625/psv:P625 ?node.\n",
        "?node wikibase:geoLatitude ?capital_lat.\n",
        "?node wikibase:geoLongitude ?capital_lon.\n",
        "```\n",
        "Mediante p:P625/psv:P625 se pasa al valor del nodo de las coordenadas de la localización (P625)y con `wikibase:geoLatitude` y  `wikibase:geoLongitude` se recuperan la latitud y la longitud.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEMZwkQ4t6-2"
      },
      "source": [
        "q = '''\n",
        "SELECT  \n",
        "  ?countryLabel ?capitalLabel ?population \n",
        "  ?capital_lon ?capital_lat\n",
        "  ?mayorLabel \n",
        "WHERE {\n",
        "  # TODO\n",
        "  \n",
        "  \n",
        "  # Coordenadas\n",
        "  \n",
        "\n",
        "  SERVICE wikibase:label { \n",
        "    bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". \n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "capital_df = dataframe_results (wd_sparql, '', q)\n",
        "capital_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expTqbrIpZaC"
      },
      "source": [
        "Convertir los datos numéricos al formato adecuado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzcRUu1UuLjz"
      },
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "capital_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa5vOb8KqirD"
      },
      "source": [
        "Eliminar datos duplicados (si los hay)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7dCuV91qtoD"
      },
      "source": [
        "len(capital_df['capitalLabel'].unique()) == len (capital_df)\n",
        "\n",
        "# TODO\n",
        "\n",
        "capital_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg7Eb1sOuVIa"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "capital_df['population'].sort_values().plot(kind='barh', color='C0', title='Population')\n",
        "plt.ylabel('')\n",
        "plt.ticklabel_format(style='plain', axis='x')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQgwuaQPxHIZ"
      },
      "source": [
        "import folium\n",
        "\n",
        "euro_map = folium.Map(location=[50,0],tiles=\"OpenStreetMap\", zoom_start=4)\n",
        "\n",
        "for p in range ( capital_df.shape[0]):\n",
        "  lat = # TODO\n",
        "  lon = # TODO\n",
        "  name = # TODO\n",
        "  folium.Marker ( [ , ], # TODO\n",
        "                       tooltip = name\n",
        "                       ).add_to(euro_map)\n",
        "\n",
        "euro_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z42J6JxYtXAj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0NNavx5gl8"
      },
      "source": [
        "# 4. Grafos : Conexiones aéreas\n",
        "\n",
        "En esta sección vamos a estudiar los grafos aplicados a las rutas aéreas. La idea es observar las rutas entre aeropuertos y encontrar vuelos entre ellos.\n",
        "\n",
        "Vamos a utilizar dos conjuntos de datos para el análisis:\n",
        "\n",
        "\n",
        "1.   **Aeropuertos**: Datos sobre los aeropuertos identificados por su código IATA\n",
        "2.   **Rutas**: Datos sobre rutas aéreas con origen y destino aeropuertos y vuelos entre ellos.\n",
        "\n",
        "Para obtener estos datos usaremos la URL `http://api.travelpayouts.com` que devuelve los datos en formato JSON. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdLLza7vxHCo"
      },
      "source": [
        "def get_airport_data():\n",
        "    url = 'https://api.travelpayouts.com/data/en/airports.json'\n",
        "    with urllib.request.urlopen(url) as url:\n",
        "        airport_json = json.loads(url.read().decode(\"utf-8\"))\n",
        "    return airport_json\n",
        "\n",
        "def get_routes_data():\n",
        "    url = \"http://api.travelpayouts.com/data/routes.json\"\n",
        "    with urllib.request.urlopen(url) as url:\n",
        "        data = json.loads(url.read().decode(\"utf-8\"))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rgGoR73pwE"
      },
      "source": [
        "Los datos de los aeropuertos son el código IATA, nombre, ciudad, etc.\n",
        "\n",
        "Solamente se van a utilizar ciertos atributos de los aeropuertos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i0dorT-xG_j"
      },
      "source": [
        "airport_json = # TODO\n",
        "\n",
        "columns = ['city_code', 'country_code','name','code']\n",
        "# TODO : convertir a dataframe\n",
        "airport_df = \n",
        "airport_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYpqDq9o5CSP"
      },
      "source": [
        "A continuación hay que filtrar los datos pues sólo vamos a analizar los aeropuertos de EE.UU. y de España:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMq-tcJTxG8b"
      },
      "source": [
        "airport_us = # TODO\n",
        "\n",
        "airport_es = # TODO\n",
        "airport_es.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRw_61ay5lGJ"
      },
      "source": [
        "Se necesita unas variables que contengan la lista de códigos de los aeropuertos que usaremos más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4VBVKi0xG6W"
      },
      "source": [
        "airport_us_in =  # TODO\n",
        "airport_es_in =  # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipEUezI354L9"
      },
      "source": [
        "Lo siguiente es cargar los datos de las rutas consistentes en códigos IATA de los aeropuertos, conexiones y detalles del vuelo, aunque sólo son de interés ciertas columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNHWj3I2xG4n"
      },
      "source": [
        "routes_json =  # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg3gl3QVxG0Y"
      },
      "source": [
        "columns = ['departure_airport_iata', 'arrival_airport_iata']\n",
        "\n",
        "# TODO: convertir a dataframe\n",
        "routes_df = \n",
        "routes_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJAPzY-u-42U"
      },
      "source": [
        "Se necesita una columna más que cuente el número de vuelos entre dos aeropuertos. En el JSON de rutas hay un array con `planes` y lo que se necesita es el número de elementos de ese array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiPjyk72xGty"
      },
      "source": [
        "routes_df['flights'] =  # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQO8T0fdB2s-"
      },
      "source": [
        "Para realizar el análisis hay que filtrar las rutas con origen y destino EE.UU. y con origen y destino España."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re-HkMU8xGrr"
      },
      "source": [
        "# TODO: Filtrar rutas con origen y destino USA\n",
        "routes_us_f = \n",
        "\n",
        "# TODO: Filtrar rutas con origen y destino España\n",
        "routes_es_f = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiMmFNdmxGo4"
      },
      "source": [
        "routes_es_f.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha0BMYWjCuFE"
      },
      "source": [
        "Hay que contar el número de rutas entre dos aeropuertos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuEwAkjnxGnO"
      },
      "source": [
        "#TODO: Contar las rutas entre 2 aeropuertos\n",
        "\n",
        "routes_us_g = # TODO\n",
        "\n",
        "routes_es_g = # TODO\n",
        "\n",
        "routes_es_g.head(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUtNLWOQDPxa"
      },
      "source": [
        "Como se puede ver existen bastantes rutas así que hay que realizar un filtro de los aeropuertos que tengan más de 5 conexiones para el caso de EE.UU. y 3 conexiones para el caso de España."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vj6zLdoxGhL"
      },
      "source": [
        "#TODO Filtrar las rutas que tengan más de 5 conexiones USA\n",
        "routes_us_g = \n",
        "#TODO Filtrar las rutas que tengan más de 3 conexiones ESP\n",
        "routes_es_g = \n",
        "routes_es_g.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGJYvdZeEQ6m"
      },
      "source": [
        "Ahora ya está todo listo para el grafo donde los **aeropuertos** son los **nodos** y las **rutas** entre ellos son los **arcos**.\n",
        "\n",
        "Usando los dataset que han sido preparados previamente, vamos a representar los grafos usando `networkx`y `matplotlib`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVdYXMibxGSi"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "def draw_graph(data):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  g = nx.from_pandas_edgelist(data, \n",
        "                              source='departure_airport_iata', \n",
        "                              target='arrival_airport_iata')\n",
        "\n",
        "  layout = nx.spring_layout(g, iterations=50)\n",
        "\n",
        "  nx.draw_networkx_edges(g, layout, edge_color='#AAAAAA')\n",
        "\n",
        "  # TODO: obtener un array de nodos de aeropuertos de destino\n",
        "  dest = [ ] # node in g.nodes()\n",
        "\n",
        "  size = [g.degree(node) * 240 for node in g.nodes() if node in data.arrival_airport_iata.unique()]\n",
        "  nx.draw_networkx_nodes(g, layout, nodelist=dest, node_size=size, node_color='lightblue')\n",
        "\n",
        "  # TODO: obtener un array de nodos de aeropuertos de origen\n",
        "  orig = [ ] # node in g.nodes()\n",
        "\n",
        "  nx.draw_networkx_nodes(g, layout, nodelist=orig, node_size=300, node_color='#AAAAAA')\n",
        "  high_degree_orig = [node for node in g.nodes() if node in data.departure_airport_iata.unique() and g.degree(node) > 1]\n",
        "\n",
        "  nx.draw_networkx_nodes(g, layout, nodelist=high_degree_orig, node_size=300, node_color='#fc8d62')\n",
        "\n",
        "  orig_dict = dict(zip(orig, orig))\n",
        "  nx.draw_networkx_labels(g, layout, labels=orig_dict)\n",
        "  \n",
        "  plt.axis('off')\n",
        "  plt.title(\"Connections\")  \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "draw_graph(routes_es_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB13W1KtGIEP"
      },
      "source": [
        "Los nodos en azul son conexiones de origen y en naranja las conexiones de destino. El tamaño de los círculos indica el número de conexiones con origen y destino al nodo. Según el grafo anterior deberían observarse los principales aeropuertos españoles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNFzk93EFnx-"
      },
      "source": [
        "draw_graph(routes_us_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKLp7NZiHBgX"
      },
      "source": [
        "En el caso de EE.UU. se pueden ver los principales aeropuertos: ORD, DEN y LAX. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ucAgFIPHcIO"
      },
      "source": [
        "A continuación vamos a definir diferentes métricas de centralidad que representan los factores de proximidad entre nodos en un grafo.\n",
        "\n",
        "La primera es el **grado de centralidad** que mide para cada nodo del grafo el número de conexiones con origen y destino a ese nodo:\n",
        "\n",
        "$ C_{D} (i) = \\sum_{ j=1 \\\\ (i \\neq j)}^{N}x_{ij} $\n",
        "\n",
        "\n",
        "La librería `networkx` dispone de una función para calcular el grado de centralidad de una grafo: `degree_centrality`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZJgRbm04X24"
      },
      "source": [
        "def show_centrality(data):\n",
        "\n",
        "  g = nx.from_pandas_edgelist(data, \n",
        "                              source='departure_airport_iata', \n",
        "                              target='arrival_airport_iata')\n",
        "  # TODO: calcular \n",
        "  deg_cen = \n",
        "  data_deg_cen = pd.DataFrame(deg_cen.items())\n",
        "  data_deg_cen = data_deg_cen[data_deg_cen[1] > 0.05]\n",
        "  # plot the histogram \n",
        "  plt.barh(data_deg_cen[0], data_deg_cen[1])\n",
        "  plt.xlabel('Airports')\n",
        "  plt.ylabel('Degree Centrality')\n",
        "  plt.show()\n",
        "\n",
        "show_centrality(routes_es_g)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeVXVsJ7OCrc"
      },
      "source": [
        "show_centrality(routes_us_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNDh-_XKLzWm"
      },
      "source": [
        "La otra métrica que vamos a usar es la **centralidad de intermediación** (betweenness centrality) que determina el número de rutas que pasan a través de un nodo en una red. Significa que puede haber un número de conexiones entre dos nodos a través de un nodo específico.\n",
        "\n",
        "$ C_{B} (i) = \\sum_{ j=1 \\\\ (i \\neq j)}^{N}  \\sum_{ k=1 \\\\ (k \\neq i)}^{j-1} \\frac {g_{jk}(i)}{g_{jk}} $\n",
        "\n",
        "También la librería `networkx` dispone de una función para calcular el grado de centralidad de una grafo: `betweenness_centrality`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRAfJS823PGw"
      },
      "source": [
        "def show_bet_centrality(data):\n",
        "  g = nx.from_pandas_edgelist(data, source='departure_airport_iata', target='arrival_airport_iata')\n",
        "  bet_cen =  # TODO: calcular centralidad de intermediacion\n",
        "  data_bet_cen = pd.DataFrame(bet_cen.items())\n",
        "  # print(data)\n",
        "  data_bet_cen = data_bet_cen[data_bet_cen[1] > 0.05]\n",
        "  plt.bar(data_bet_cen[0], data_bet_cen[1])\n",
        "  plt.xlabel('Airports')\n",
        "  plt.ylabel('Betweenness Centrality')\n",
        "  plt.show()\n",
        "\n",
        "show_bet_centrality( routes_es_g )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AImzaEn8N1o1"
      },
      "source": [
        "show_bet_centrality( routes_us_g )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRGJ5qpixF1B"
      },
      "source": [
        "Estos aeropuertos con más conexiones a través de ellos son los que más opciones ofrecen para las conexiones con otros destinos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhxfJ69UWLUw"
      },
      "source": [
        "## 4.1. Consulta de API: vuelos en tiempo real\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTYw73ExOlzX"
      },
      "source": [
        "En esta sección vamos a trabajar con datos sobre rutas aéreas, compañías aéreas y aeropuertos.\n",
        "\n",
        "Los datos en tiempo real los vamos obtener de la [API OpenSky](https://opensky-network.org/). Lo primero será cargar las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9COapkx5hYE"
      },
      "source": [
        "!git clone https://github.com/openskynetwork/opensky-api.git\n",
        "!pip install -e ./opensky-api/python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UevA0Vvg7PoQ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('./opensky-api/python')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPsZsBecPO5H"
      },
      "source": [
        "Con esta API vamos a obtener los datos de los vuelos que están en este instante sobrevolando la península ibérica:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfd9pp7j6GgV"
      },
      "source": [
        "from opensky_api import OpenSkyApi\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "api = OpenSkyApi()\n",
        "\n",
        "t = int(time.time())\n",
        "\n",
        "bboxSpain = [27.4335426 ,\t43.9933088 ,\t-18.3936845, \t4.5918885]\n",
        "\n",
        "states = api.get_states(time_secs= t, bbox=bboxSpain)\n",
        "for s in states.states[0:10]:\n",
        "  print(\"(%r, %r, %r, %r)\" % (s.longitude, s.latitude, s.velocity, s.callsign ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tIWdfolPjUa"
      },
      "source": [
        "Y esto lo vamos a representar en un mapa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE0CUpb4GPLN"
      },
      "source": [
        "import folium\n",
        "\n",
        "world_map = folium.Map(location =[40.416775, -3.7388], tiles='stamenterrain', zoom_start=6,prefer_canvas=True)\n",
        "\n",
        "for s in states.states[0:100]:\n",
        "  try:\n",
        "    folium.Marker([ , ], # TODO: lat lon\n",
        "                tooltip=s.callsign + ' (' + s.origin_country + ')',\n",
        "                icon=folium.Icon(icon='plane', color='green')\n",
        "              ).add_to(world_map)\n",
        "  except:\n",
        "    pass\n",
        "world_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS7Q2_kD2dsG"
      },
      "source": [
        "# Parte 5. Grafos de conocimiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_NJmxE6XGoW"
      },
      "source": [
        "Un **[grafo de conocimiento](https://en.wikipedia.org/wiki/Knowledge_Graph)** consiste en una colección de información interrelacionada, normalmente limitada a un dominio específico y gestionado como un grafo. La unidad básica de un grafo de conocimiento es una tripleta *subjeto-predicado-objeto*, que en ocasiones se denota por (*head*, *relation*, *tail*) o más conciso (h, r, t).\n",
        "\n",
        "\n",
        "El grafo de conocimiento representa el conocimiento mediante *entidades* (como personas, localizaciones, organizaciones, incluso eventos) y sus relaciones. Cada tripleta define una conexión entre dos entidades en el grafo. Una ontología define las relaciones y entidades aceptables en el grafo.\n",
        "\n",
        "\n",
        "Veamos un ejemplo: Si un nodoA = `Putin` y el nodoB = `Rusia`, entonces es bastante probable que el arco que los une sea `presidenteDe`. Un nodo o entidad puede tener múltiples relaciones, así Putin no sólo es presidente de Rusia también trabajó para el servicio secreto de la Unión Soviética o KGB.\n",
        "\n",
        "La idea es la misma que la web semántica: hacer que toda la información en internet esté conectada y sea entendible por los ordenadores mediante estándares como `RDF` o `schema.org`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAxylTLNJ3-"
      },
      "source": [
        "Es posible construir un grafo de conocimiento a partir de texto, pero para ello hay que hacer que una máquina \"entienda\" el lenguaje natural. Esto se puede aproximar mediante las técnicas de **Procesamiento del Lenguaje Natural** (NLP por sus siglas en inglés) como segmentación de sentencias, análisis de dependencias, etiquetado del discurso y reconocimiento de entidades. Veremos una introdución a todo esto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Goos-ohrNZuy"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlolUqqMVjV"
      },
      "source": [
        "## 5.1 Extracción de entidades\n",
        "\n",
        "El primer paso para construir un grafo de conocimiento es separar el texto de los documentos o artículos en sentencias u oraciones. De todas ellas nos vamos a centrar en aquellas que tienen exactamente 1 sujeto y 1 objeto.\n",
        "\n",
        "Cada sentencia se divide en tokens, cuando `spaCy` procesa el texto, añade un tag **`dep_`** a cada palabra indicando la función : sujeto, objeto, etc. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSlvOV5KMDlk"
      },
      "source": [
        "doc = nlp(\"London is the capital and largest city of England and the United Kingdom.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, \"...\", token.dep_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9DzMNO8Opjd"
      },
      "source": [
        "El sujeto (`nsubj`) en esta oración según el analizador de dependencias es `London`, aunque las entidades se pueden encontrar modificadas con `amond` o formar composiciones `compound`. El objeto de la sentencia es `England` (`pobj`) junto con `United Kingdom`. Por tanto, habría que extraer tanto el sujeto como el objeto junto con sus modificadores. Es conveniente ver la documentación de SpaCy para ver los diferentes tipos de tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dsy4Iyb91Xq"
      },
      "source": [
        "Existe sin embargo un API (Wikifier API) que permite etiquetar entidades en un texto. Como ejemplo de lo que puede hacer la API Wikifier, supongamos el siguiente texto sobre [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk) , extraído y modificado de la WikiPedia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiondganZUyB"
      },
      "source": [
        "EM_text = \"\"\"\n",
        "Elon Musk is a business magnate, industrial designer, and engineer. \n",
        "Elon Musk is the founder, CEO, CTO, and chief designer of SpaceX. \n",
        "Elon Musk is also early investor, CEO, and product architect of Tesla, Inc. \n",
        "Elon Musk is also the founder of The Boring Company and the co-founder of Neuralink. \n",
        "A centibillionaire, Musk became the richest person in the world in January 2021, with an estimated net worth of $185 billion at the time, surpassing Jeff Bezos. \n",
        "Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. \n",
        "Elon Musk briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. \n",
        "Elon Musk transferred to the University of Pennsylvania two years later, where Elon Musk received dual bachelor's degrees in economics and physics. \n",
        "Elon Musk moved to California in 1995 to attend Stanford University, but decided instead to pursue a business career. \n",
        "Elon Musk went on co-founding a web software company Zip2 with Elon Musk brother Kimbal Musk.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6IwYAfl-3II"
      },
      "source": [
        "Definimos una función que nos permita acceder a Wikifier API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFpkC_uf87Yy"
      },
      "source": [
        "import urllib\n",
        "from string import punctuation\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
        "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
        "\n",
        "\n",
        "def wikifier(text, lang=\"en\", threshold=0.8):\n",
        "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
        "    # URL.\n",
        "    data = urllib.parse.urlencode([\n",
        "        (\"text\", text), (\"lang\", lang),\n",
        "        (\"userKey\", \"icrogulbfeovyizvqttnfhjcixksdw\"),\n",
        "        (\"pageRankSqThreshold\", \"%g\" % threshold), \n",
        "        (\"applyPageRankSqThreshold\", \"true\"),\n",
        "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
        "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
        "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
        "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
        "    ])\n",
        "    url = \"http://www.wikifier.org/annotate-article\"\n",
        "    # CALL API.\n",
        "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
        "    with urllib.request.urlopen(req, timeout=60) as f:\n",
        "        response = f.read()\n",
        "        response = json.loads(response.decode(\"utf8\"))\n",
        "    # Output.\n",
        "    results = list()\n",
        "    for annotation in response[\"annotations\"]:\n",
        "        # Filter \n",
        "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
        "\n",
        "            # \n",
        "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
        "                label = 'Person'\n",
        "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
        "                label = 'Organization'\n",
        "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
        "                label = 'Location'\n",
        "            else:\n",
        "                label = None\n",
        "\n",
        "            results.append({'title': annotation['title'], \n",
        "                            'wikiId': annotation['wikiDataItemId'], \n",
        "                            'label': label,\n",
        "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdExAeDrAu3I"
      },
      "source": [
        "Wikifier API devuelve todas las clases a las que una entidad pertenece: busca en INSTANCE_OF y SUBCLASS_OF en toda la jerarquía. Por eso hay un filtro final que sólo busca las categorías de personal, organización o localización.\n",
        "\n",
        "\n",
        "Una cosa muy interesante de este API es que se obtienen los identificadores de  WikiData de las entidades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaUgHUZh9NEI"
      },
      "source": [
        "result = wikifier (EM_text)\n",
        "for entity in result:\n",
        "  print (\"%s - %s - (%s)\" % (entity['label'], entity['title'], entity['wikiId']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKcN5ZVZBRHB"
      },
      "source": [
        "La siguiente función que se encarga de obtener el listado de entidades y las relaciones que se establecen entre ellas dentro de una sentencia. Para ello hay que tratar sentencia por sentencia, eliminar los signos de puntuación y obtener las entidades con Wikifier API. \n",
        "\n",
        "Consideramos relaciones si dos entidades aparecen en la misma sentencia. Así se van uniendo por pares a una lista. Por último, se eliminan duplicados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdjogiZZCtEE"
      },
      "source": [
        "import itertools\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    return '' # TODO\n",
        "\n",
        "def deduplicate_dict(d):\n",
        "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
        "\n",
        "\n",
        "def entities_extraction (text):\n",
        "  entities_list = list()\n",
        "  relations_list = list()\n",
        "\n",
        "  for sentence in  # TODO: recorrer sentencias\n",
        "    sentence =     # TODO: eliminar signos de puntuación\n",
        "\n",
        "    entities =     # TODO : llamar a wikifier\n",
        "    # TODO: añadir a la lista un diccionario por cada entidad con los valores de title, wikiId, label\n",
        "    entities_list.\n",
        "\n",
        "    for permutation in itertools.permutations(entities, 2): # generar relaciones \n",
        "      for source in permutation[0]['characters']:\n",
        "        for target in permutation[1]['characters']:\n",
        "          relations_list.append({'source': permutation[0]['title'],\n",
        "                                 'target': permutation[1]['title']})\n",
        "\n",
        "  return {'entities': [], 'relations': [] } # TODO : devolver un diccionario\n",
        "\n",
        "\n",
        "EM_dict = entities_extraction (EM_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92YeXqj8ESl3"
      },
      "source": [
        "EM_dict['relations']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hejU4PaQutg1"
      },
      "source": [
        "Finalmente se contruye el grafo a partir de las entidades y las relaciones. Necesitamos expresar el grafo en forma de tabla para almacenar las tripletas, un campo con el origen, otro con el predicado y otro con el destino. Para ello, vamos a usar la estructura conocida que es el Dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne0mdk7vu-fR"
      },
      "source": [
        "source = [] # TODO\n",
        "target = [] # TODO\n",
        "\n",
        "em_kg_df = pd.DataFrame({'source':source, 'target':target})\n",
        "em_kg_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3w_zdPp0LzE"
      },
      "source": [
        "Una vez están los datos en de Dataframe, podemos directamente volcarlo a un grafo usando la `networkx` que permite usar un dataframe como origen de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yUKCVFJwY6o"
      },
      "source": [
        "G=nx.from_pandas_edgelist() # TODO "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr8UWZcXwcSf"
      },
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}