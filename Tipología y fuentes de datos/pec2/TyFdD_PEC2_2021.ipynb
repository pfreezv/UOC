{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dISYzX4ob2l7"
      },
      "source": [
        "# PEC 2. Introducción a los sistemas de recuperación de información.\n",
        "\n",
        "En esta PEC vamos a desarrollar un sistema de recuperación de información básico. Partiendo de una lista de documentos de texto tendrás que usar las técnicas de recuperación de información vistas en la asignatura para obtener, procesar y analizar datos útiles a partir del contenido.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#TODO: nombre y apellidos\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oio_-Jm3K_kJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Parte 1**. Tareas básicas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED51nVLeFy-v"
      },
      "source": [
        "Además de las ya clásicas `pandas` y `numpy`, vamos a utilizar la librería [NTLK](https://es.wikipedia.org/wiki/NLTK) (Natural Language Toolkit), una librería Python utilizada para analizar texto y aprendizaje automático."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (1.20.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\aland\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.6.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: regex in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (2021.7.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (4.61.2)\n",
            "Requirement already satisfied: click in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (8.0.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\aland\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from click->nltk) (0.4.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\aland\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZSx3piMEFn-4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\aland\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\aland\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jFbru22JeCt"
      },
      "source": [
        "## 1.1 Separando las palabras (Tokenization)\n",
        "\n",
        "El significado de cada sentencia se obtiene de las palabra que contiene. Así que analizando las palabras presentes en un texto se puede interpretar el significado. Así que lo primero que hay que hacer para poder tratar el texto es separar las palabras que lo componen, es decir, hacer una lista  de palabras. El modelo que vamos a utilizar aquí se denomina [**bolsa-de-palabras**](https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras) (bag-of-words) ya que nos interesan las palabras sin importar su posición o importancia en el documento.\n",
        "\n",
        "\n",
        "La separación de las palabres o tokenization consiste en separar el texto en palabras, también denominados tokens. Generalmente el `espacio`se utiliza para separar palabras y elementos como los puntos, comas, dos puntos, etc. se utilizan para separar sentencias.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK4PwSVyKU3D"
      },
      "source": [
        "Hay multiples formas de realizar la separación de palabras para un texto dado. \n",
        "\n",
        "### 1.1.1 Funciones de Python\n",
        "\n",
        "Se puede usar la función `split()` para separar una cadena de texto en una lista de palabras. Por defecto `split()` utiliza el espacio en blanco, aunque se puede usar cualquier caracter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 449,
      "metadata": {
        "id": "KuK5q-MOKf0q"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
            ]
          },
          "execution_count": 449,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text01 = \"Children shouldn't drink a sugary drink before bed.\"\n",
        "text01.split(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuZQafeDKiTj"
      },
      "source": [
        "El método `split()` de Python no considera los signos de puntuación como elementos separados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsTbr0lQvBtQ"
      },
      "source": [
        "### 1.1.2 Expresiones regulares\n",
        "\n",
        "El módulo `re` ofrece un conjunto de funciones para buscar coincidencias en una cadena de texto. Una *expresión regular* es una secuencia de caracteres que definen un patrón de búsqueda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "id": "b-D5UYVfxW9X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language,', 'library', 'and', 'purpose', 'of', 'modeling.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text02 = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on language, library and purpose of modeling.\"\"\"\n",
        "tokens = re.split(r\"\\s\", text02)  # TODO: usar una expresión regular para separar las palabras\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9s_37f8xtCT"
      },
      "source": [
        "### 1.1.3 Con NLTK\n",
        "\n",
        "Natural Language Toolkit (NLTK) tiene la función `word_tokenize()` para separación de palabras y `sent_tokenize()` para separación de sentencias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {
        "id": "x8t62gvCKrkD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<', 'p', '>', 'This', 'is', 'the', 'first', 'sentence', '.']\n",
            "['A', 'gallon-of-milk', 'in', 'the', 'U.S.', 'costs', '$', '2.99', '.']\n",
            "['Is', 'this', 'the', 'third', 'sentence', '?']\n",
            "['Yes', ',', 'it', 'is', '!', '<', '/p', '>']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text03 = \"<p>This is the first sentence. A gallon-of-milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!</p>\"\n",
        "\n",
        "for sent in sent_tokenize(text03): # TODO: separar las sentencias \n",
        "  print (word_tokenize(sent)) # TODO: separar las palabras  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIJt7ZFTDQSY"
      },
      "source": [
        "## 1.2. Eliminación de números y símbolos. Conversión a minúsculas.\n",
        "\n",
        "Como se puede comprobar, `word_tokenizer()` mantiene los signos de puntuación así como los los números y otros símbolos.\n",
        "\n",
        "Una estrategia para reducir el número de palabras/tokens es convertirlas a minúsculas, pues algunos signos de puntuación modifican la letra inicial de las palabras. Así se consigue reducir el número de variantes de una misma palabra.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {
        "id": "42rTEi1l-3SR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'the', 'first', 'sentence', 'a', 'gallon', 'of', 'milk', 'in', 'the', 'us', 'costs', 'is', 'this', 'the', 'third', 'sentence', 'yes', 'it', 'is']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "def remove_tags (s):\n",
        " \n",
        "  p = re.compile('<.*?>')  # TODO: eliminar tags html\n",
        "  return re.sub(p,'', s)  # TODO\n",
        "\n",
        "# TODO : eliminar tags, tokenizar, convertir a minúsculas y eliminar símbolos no alfabéticos y números \n",
        "def tokenize_and_remove_punctuations(s):\n",
        "    remove_tag = remove_tags(s)\n",
        "    simbol_to_remove = re.compile('[,.!?$0-9_/=]')\n",
        "    \n",
        "    removed_text = re.sub(simbol_to_remove,'', remove_tag)\n",
        "    removed_text = re.sub(r':?\\\\+', '\\n', removed_text)\n",
        "    return word_tokenize((removed_text.lower()).replace(\"-\", \" \"))\n",
        "\n",
        "\n",
        "print (tokenize_and_remove_punctuations (text03))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9I2sR40MnSQ"
      },
      "source": [
        "## 1.3. Palabras vacías\n",
        "\n",
        "Las *palabras vacías* (stopwords) son palabras más comunes en cualquier lenguaje, tienen sentido gramatical pero con poco significado para el análisis de un texto. Estas palabras vacías se incluyen artículos, preposiciones, conjunciones, pronombres, etc. así que su eliminación reduce considerablemente el número de palabras.\n",
        "\n",
        "NLTK tiene listas de palabras vacías en 16 idiomas. En este caso, se ha cargado la lista en inglés.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {
        "id": "jcrShuseB4ip"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 453,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO : dada una lista de tokens, suprimir aquellos que sean palabras vacías o que su longitud es menor o igual 2\n",
        "\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    filtered_words = [x for x in tokens if x.lower() not in stopwords and len(x) > 2]  # TODO\n",
        "    return filtered_words\n",
        "\n",
        "\n",
        "len (remove_stop_words ( tokenize_and_remove_punctuations (text03)  ) ) == 8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2VeHy1uJqiV"
      },
      "source": [
        "## 1.4. Normalización\n",
        "\n",
        "Muchos idiomas contienen palabras derivadas de otras y esto se denomina [flexión](https://es.wikipedia.org/wiki/Flexi%C3%B3n_(ling%C3%BC%C3%ADstica)). La flexión es la modificación de una para expresar diferentes categorías gramaticales como persona, número, género, etc.\n",
        "\n",
        "Tratar esta flexión para llevar las palabras a una forma base se denomina **normalización de palabras**. La normalización permite que si se busca por una palabra se haga al mismo tiempo por todas sus flexiones.\n",
        "\n",
        "La **lematización** es el proceso de reducir la inflexión de las palabras para llevarla a su forma origen o raíz. El lema es la parte de la palabra a la que se añade la flexión.\n",
        "\n",
        "En NLTK hay disponibles diferentes lematizadores aunque aquí vamos a utilizar el más conocido: [algoritmo de Porter](https://es.wikipedia.org/wiki/Algoritmo_de_Porter). \n",
        "\n",
        "\n",
        "Podéis ver algo más de estos procesos en \n",
        "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {
        "id": "qI7x5BOHJqG_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['first', 'sentenc', 'gallon', 'milk', 'cost', 'third', 'sentenc', 'ye']"
            ]
          },
          "execution_count": 454,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_words(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    # TODO : obtener la versión básica de todos los tokens\n",
        "    stemmed_words = [stemmer.stem(x) for x in tokens]\n",
        "    return stemmed_words\n",
        "\n",
        "\n",
        "stem_words ( remove_stop_words(tokenize_and_remove_punctuations (text03)) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88Wpw_m2afn"
      },
      "source": [
        "Una vez que ya están realizadas las operaciones básicas sobre el texto, es momento de ponerlo todo junto en la función `preprocess_data ()` que recibe un array de pares `(documentId, text)` y aplica las transformaciones anteriormente descritas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {
        "id": "CV3V7b68FTa6"
      },
      "outputs": [],
      "source": [
        "def preprocess_text ( text , stem=True):\n",
        "    tokens = tokenize_and_remove_punctuations (text)        # TODO : tokenizar y eliminar puntuación\n",
        "    filtered_tokens = remove_stop_words(tokens)     # TODO : eliminar palabras vacías\n",
        "    if stem :\n",
        "        stemmed_tokens = stem_words(filtered_tokens)    # TODO : normalizar\n",
        "    else:\n",
        "        stemmed_tokens=filtered_tokens  # TODO\n",
        "    return stemmed_tokens\n",
        "\n",
        "def preprocess_data(contents, stem=True):\n",
        "    dataDict = {}\n",
        "    for content in contents:\n",
        "        dataDict[content[0]] = preprocess_text(content[1], stem)\n",
        "    return dataDict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {
        "id": "8SLxkerz5FwZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'doc01': ['love', 'watch', 'movi', 'cold', 'outsid'],\n",
              " 'doc02': ['toy', 'stori', 'best', 'anim', 'movi', 'ever', 'love'],\n",
              " 'doc03': ['watch', 'horror', 'movi', 'alon', 'night', 'realli', 'scari'],\n",
              " 'doc04': ['love',\n",
              "  'watch',\n",
              "  'film',\n",
              "  'fill',\n",
              "  'suspens',\n",
              "  'unexpect',\n",
              "  'plot',\n",
              "  'twist'],\n",
              " 'doc05': ['mom',\n",
              "  'love',\n",
              "  'watch',\n",
              "  'movi',\n",
              "  'dad',\n",
              "  'hate',\n",
              "  'movi',\n",
              "  'theater',\n",
              "  'brother',\n",
              "  'like',\n",
              "  'kind',\n",
              "  'movi',\n",
              "  \"n't\",\n",
              "  'watch',\n",
              "  'singl',\n",
              "  'movi',\n",
              "  'sinc',\n",
              "  'got',\n",
              "  'colleg']}"
            ]
          },
          "execution_count": 456,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_1 = \"I love watching movies when it's cold outside ;-)\"\n",
        "document_2 = \"Toy Story is the best animation movie ever, I love it!\"\n",
        "document_3 = \"Watching horror movies alone at night is really scary\"\n",
        "document_4 = \"He loves to watch films filled with suspense and unexpected plot twists\"\n",
        "document_5 = \"My mom loves to watch movies. My dad hates movie theaters. My brothers like any kind of movie. And I haven't watched a single movie since I got into college\"\n",
        "documents = [document_1, document_2, document_3, document_4, document_5]\n",
        "\n",
        "docIds = ['doc01','doc02','doc03','doc04','doc05']\n",
        "\n",
        "documents_list = zip(docIds,documents)# TODO : generar una lista de la forma  [('doc01': 'I love..'), ... ]\n",
        "\n",
        "\n",
        "data_docs = preprocess_data(documents_list)  # TODO : preprocesar la lista de documentos de ejemplo\n",
        "data_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpDoPJNOJmwE"
      },
      "source": [
        "## 1.5. Frecuencia de las palabras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdgkCF-rRn88"
      },
      "source": [
        "Ahora vamos a ver cómo de importante es una palabra/token en los documentos. \n",
        "\n",
        "Lo primero que hay que hacer es obtener un vocabulario que no es más que la lista de todos los tokens únicos que aparecen en todos los documentos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {
        "id": "ap24675u5Tmw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['alon',\n",
              " 'anim',\n",
              " 'best',\n",
              " 'brother',\n",
              " 'cold',\n",
              " 'colleg',\n",
              " 'dad',\n",
              " 'ever',\n",
              " 'fill',\n",
              " 'film',\n",
              " 'got',\n",
              " 'hate',\n",
              " 'horror',\n",
              " 'kind',\n",
              " 'like',\n",
              " 'love',\n",
              " 'mom',\n",
              " 'movi',\n",
              " \"n't\",\n",
              " 'night',\n",
              " 'outsid',\n",
              " 'plot',\n",
              " 'realli',\n",
              " 'scari',\n",
              " 'sinc',\n",
              " 'singl',\n",
              " 'stori',\n",
              " 'suspens',\n",
              " 'theater',\n",
              " 'toy',\n",
              " 'twist',\n",
              " 'unexpect',\n",
              " 'watch']"
            ]
          },
          "execution_count": 457,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_vocabulary(data):\n",
        "    tokens = []\n",
        "    for token_list in data.values():\n",
        "        tokens=tokens +token_list # TODO\n",
        "\n",
        "    fdist={}\n",
        "    for word in sorted(set(tokens)):    \n",
        "        \n",
        "        fdist[word] = tokens.count(word)\n",
        "    return list(fdist.keys())\n",
        "    \n",
        "\n",
        "get_vocabulary (data_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS802HVsRsQp"
      },
      "source": [
        "Para ello podemos calcular la frecuencia de cada término contando el número de veces que aparece en cada documento, que será una medida de su peso o importancia.\n",
        "\n",
        "$TF (t,d) = f_{t,d}$  (#número de repeticiones del término $t$ en el documento $d$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {
        "id": "mvPko3Wm7Y8o"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'brother': 1,\n",
              " 'colleg': 1,\n",
              " 'dad': 1,\n",
              " 'got': 1,\n",
              " 'hate': 1,\n",
              " 'kind': 1,\n",
              " 'like': 1,\n",
              " 'love': 1,\n",
              " 'mom': 1,\n",
              " 'movi': 4,\n",
              " \"n't\": 1,\n",
              " 'sinc': 1,\n",
              " 'singl': 1,\n",
              " 'theater': 1,\n",
              " 'watch': 2}"
            ]
          },
          "execution_count": 458,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "def calculate_tf(tokens):\n",
        "    tf_score = {}\n",
        "    for token in sorted(tokens):\n",
        "        tf_score[token] = FreqDist(tokens)[token] # TODO: calcular tf\n",
        "    \n",
        "    return tf_score\n",
        "\n",
        "fdist = calculate_tf ( data_docs['doc05'])\n",
        "\n",
        "fdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4X4luHToWR"
      },
      "source": [
        "La **frecuencia inversa de documentos** para un término $t$ es el logaritmo (en este caso en base 2) del cociente entre el número de documentos y el número de documentos en los que aparece el término $t$.\n",
        "\n",
        "\n",
        "$ IDF (t) = log_{2} \\frac{N}{\\{d \\in D : t \\in d \\}} $\n",
        "\n",
        "A mayor puntuación de TF*IDF el término es más específico y a menor puntuación, más genérico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {
        "id": "ebOdwtTY6T6h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alon': 2.321928094887362,\n",
              " 'anim': 2.321928094887362,\n",
              " 'best': 2.321928094887362,\n",
              " 'brother': 2.321928094887362,\n",
              " 'cold': 2.321928094887362,\n",
              " 'colleg': 2.321928094887362,\n",
              " 'dad': 2.321928094887362,\n",
              " 'ever': 2.321928094887362,\n",
              " 'fill': 2.321928094887362,\n",
              " 'film': 2.321928094887362,\n",
              " 'got': 2.321928094887362,\n",
              " 'hate': 2.321928094887362,\n",
              " 'horror': 2.321928094887362,\n",
              " 'kind': 2.321928094887362,\n",
              " 'like': 2.321928094887362,\n",
              " 'love': 0.32192809488736235,\n",
              " 'mom': 2.321928094887362,\n",
              " 'movi': 0.32192809488736235,\n",
              " \"n't\": 2.321928094887362,\n",
              " 'night': 2.321928094887362,\n",
              " 'outsid': 2.321928094887362,\n",
              " 'plot': 2.321928094887362,\n",
              " 'realli': 2.321928094887362,\n",
              " 'scari': 2.321928094887362,\n",
              " 'sinc': 2.321928094887362,\n",
              " 'singl': 2.321928094887362,\n",
              " 'stori': 2.321928094887362,\n",
              " 'suspens': 2.321928094887362,\n",
              " 'theater': 2.321928094887362,\n",
              " 'toy': 2.321928094887362,\n",
              " 'twist': 2.321928094887362,\n",
              " 'unexpect': 2.321928094887362,\n",
              " 'watch': 0.32192809488736235}"
            ]
          },
          "execution_count": 459,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math \n",
        "\n",
        "def calculate_idf(data):\n",
        "    idf_score = {}\n",
        "    N = len(data)        # TODO: número de documentos\n",
        "    all_words = get_vocabulary(data)# TODO: obtener el vocabulario\n",
        "    for word in all_words:\n",
        "        word_count = 0\n",
        "        \n",
        "        for token_list in data.values():\n",
        "            if word in token_list:\n",
        "                word_count = word_count +1 \n",
        "        idf_score[word] = math.log(N/word_count,2)# TODO: calcular idf\n",
        "    return idf_score\n",
        "\n",
        "idf_score = calculate_idf ( data_docs)\n",
        "idf_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([('doc01', ['love', 'watch', 'movi', 'cold', 'outsid']), ('doc02', ['toy', 'stori', 'best', 'anim', 'movi', 'ever', 'love']), ('doc03', ['watch', 'horror', 'movi', 'alon', 'night', 'realli', 'scari']), ('doc04', ['love', 'watch', 'film', 'fill', 'suspens', 'unexpect', 'plot', 'twist']), ('doc05', ['mom', 'love', 'watch', 'movi', 'dad', 'hate', 'movi', 'theater', 'brother', 'like', 'kind', 'movi', \"n't\", 'watch', 'singl', 'movi', 'sinc', 'got', 'colleg'])])"
            ]
          },
          "execution_count": 460,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_docs.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 461,
      "metadata": {
        "id": "PnSwuFHxDwkt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'doc01': {'cold': 2.321928094887362,\n",
              "  'love': 0.32192809488736235,\n",
              "  'movi': 0.32192809488736235,\n",
              "  'outsid': 2.321928094887362,\n",
              "  'watch': 0.32192809488736235},\n",
              " 'doc02': {'anim': 2.321928094887362,\n",
              "  'best': 2.321928094887362,\n",
              "  'ever': 2.321928094887362,\n",
              "  'love': 0.32192809488736235,\n",
              "  'movi': 0.32192809488736235,\n",
              "  'stori': 2.321928094887362,\n",
              "  'toy': 2.321928094887362},\n",
              " 'doc03': {'alon': 2.321928094887362,\n",
              "  'horror': 2.321928094887362,\n",
              "  'movi': 0.32192809488736235,\n",
              "  'night': 2.321928094887362,\n",
              "  'realli': 2.321928094887362,\n",
              "  'scari': 2.321928094887362,\n",
              "  'watch': 0.32192809488736235},\n",
              " 'doc04': {'fill': 2.321928094887362,\n",
              "  'film': 2.321928094887362,\n",
              "  'love': 0.32192809488736235,\n",
              "  'plot': 2.321928094887362,\n",
              "  'suspens': 2.321928094887362,\n",
              "  'twist': 2.321928094887362,\n",
              "  'unexpect': 2.321928094887362,\n",
              "  'watch': 0.32192809488736235},\n",
              " 'doc05': {'brother': 2.321928094887362,\n",
              "  'colleg': 2.321928094887362,\n",
              "  'dad': 2.321928094887362,\n",
              "  'got': 2.321928094887362,\n",
              "  'hate': 2.321928094887362,\n",
              "  'kind': 2.321928094887362,\n",
              "  'like': 2.321928094887362,\n",
              "  'love': 0.32192809488736235,\n",
              "  'mom': 2.321928094887362,\n",
              "  'movi': 1.2877123795494494,\n",
              "  \"n't\": 2.321928094887362,\n",
              "  'sinc': 2.321928094887362,\n",
              "  'singl': 2.321928094887362,\n",
              "  'theater': 2.321928094887362,\n",
              "  'watch': 0.6438561897747247}}"
            ]
          },
          "execution_count": 461,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def calculate_tfidf(data, idf_score):\n",
        "    scores = {}\n",
        "    for key,value in data.items():\n",
        "        scores[key] = calculate_tf(value) #  TODO: calcular tf\n",
        "    for doc,tf_scores in scores.items(): \n",
        "        listed={}    # TODO\n",
        "        for token, score in tf_scores.items(): # TODO\n",
        "            listed[token] = score*idf_score[token]\n",
        "    #         \n",
        "        scores[doc] = listed\n",
        "            \n",
        "    # #          # TODO\n",
        "    # #          # TODO: calcular tf*idf\n",
        "    return scores\n",
        "    \n",
        "tfidf_score = calculate_tfidf ( data_docs, idf_score)\n",
        "tfidf_score\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keFHyYR8Ueww"
      },
      "source": [
        "## 1.6. Generar el espacio vectorial\n",
        "\n",
        "Usando las funciones anteriores vamos a construir la matriz de documentos (en filas) y términos (en columnas). Para facilitar la tarea usaremos una estructura de datos ya conocida, el **dataframe**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "metadata": {
        "id": "JRFap5kzUls0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alon</th>\n",
              "      <th>anim</th>\n",
              "      <th>best</th>\n",
              "      <th>brother</th>\n",
              "      <th>cold</th>\n",
              "      <th>colleg</th>\n",
              "      <th>dad</th>\n",
              "      <th>ever</th>\n",
              "      <th>fill</th>\n",
              "      <th>film</th>\n",
              "      <th>...</th>\n",
              "      <th>scari</th>\n",
              "      <th>sinc</th>\n",
              "      <th>singl</th>\n",
              "      <th>stori</th>\n",
              "      <th>suspens</th>\n",
              "      <th>theater</th>\n",
              "      <th>toy</th>\n",
              "      <th>twist</th>\n",
              "      <th>unexpect</th>\n",
              "      <th>watch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>doc01</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc02</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc03</th>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc04</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.321928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc05</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.643856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           alon      anim      best   brother      cold    colleg       dad  \\\n",
              "doc01  0.000000  0.000000  0.000000  0.000000  2.321928  0.000000  0.000000   \n",
              "doc02  0.000000  2.321928  2.321928  0.000000  0.000000  0.000000  0.000000   \n",
              "doc03  2.321928  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "doc04  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "doc05  0.000000  0.000000  0.000000  2.321928  0.000000  2.321928  2.321928   \n",
              "\n",
              "           ever      fill      film  ...     scari      sinc     singl  \\\n",
              "doc01  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "doc02  2.321928  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "doc03  0.000000  0.000000  0.000000  ...  2.321928  0.000000  0.000000   \n",
              "doc04  0.000000  2.321928  2.321928  ...  0.000000  0.000000  0.000000   \n",
              "doc05  0.000000  0.000000  0.000000  ...  0.000000  2.321928  2.321928   \n",
              "\n",
              "          stori   suspens   theater       toy     twist  unexpect     watch  \n",
              "doc01  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.321928  \n",
              "doc02  2.321928  0.000000  0.000000  2.321928  0.000000  0.000000  0.000000  \n",
              "doc03  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.321928  \n",
              "doc04  0.000000  2.321928  0.000000  0.000000  2.321928  2.321928  0.321928  \n",
              "doc05  0.000000  0.000000  2.321928  0.000000  0.000000  0.000000  0.643856  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "execution_count": 462,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_dataframe ( data ):\n",
        "  all_words =  get_vocabulary(data)# TODO : obtener el vocabulario\n",
        "  idf_score =  calculate_idf(data)# TODO : calcular idf\n",
        "  tf_idf_score = calculate_tfidf(data,idf_score)  # TODO : calcular if*idf\n",
        "\n",
        "  table = []\n",
        "\n",
        "  for doc in tf_idf_score.keys():\n",
        "    d={}\n",
        "    for word in all_words:\n",
        "      try:\n",
        "        d[word]=tf_idf_score[doc][word]\n",
        "      except:\n",
        "        d[word]=0\n",
        "        continue\n",
        "            \n",
        "    table.append(d)\n",
        "  \n",
        "\n",
        "  df = pd.DataFrame ( table , index = tf_idf_score.keys())\n",
        "  return df\n",
        "\n",
        "df_data = generate_dataframe (data_docs)\n",
        "df_data.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNlyfOCNFYtZ"
      },
      "source": [
        "## 1.7. Generar el índice invertido\n",
        "\n",
        "De manera similar, vamos a generar un índice invertido que almacenaremos en una estructura de datos Python: el **diccionario**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "metadata": {
        "id": "r8_AJecsFn6T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alon': [('doc03', 2.321928094887362)],\n",
              " 'anim': [('doc02', 2.321928094887362)],\n",
              " 'best': [('doc02', 2.321928094887362)],\n",
              " 'brother': [('doc05', 2.321928094887362)],\n",
              " 'cold': [('doc01', 2.321928094887362)],\n",
              " 'colleg': [('doc05', 2.321928094887362)],\n",
              " 'dad': [('doc05', 2.321928094887362)],\n",
              " 'ever': [('doc02', 2.321928094887362)],\n",
              " 'fill': [('doc04', 2.321928094887362)],\n",
              " 'film': [('doc04', 2.321928094887362)],\n",
              " 'got': [('doc05', 2.321928094887362)],\n",
              " 'hate': [('doc05', 2.321928094887362)],\n",
              " 'horror': [('doc03', 2.321928094887362)],\n",
              " 'kind': [('doc05', 2.321928094887362)],\n",
              " 'like': [('doc05', 2.321928094887362)],\n",
              " 'love': [('doc05', 0.32192809488736235)],\n",
              " 'mom': [('doc05', 2.321928094887362)],\n",
              " 'movi': [('doc05', 1.2877123795494494)],\n",
              " \"n't\": [('doc05', 2.321928094887362)],\n",
              " 'night': [('doc03', 2.321928094887362)],\n",
              " 'outsid': [('doc01', 2.321928094887362)],\n",
              " 'plot': [('doc04', 2.321928094887362)],\n",
              " 'realli': [('doc03', 2.321928094887362)],\n",
              " 'scari': [('doc03', 2.321928094887362)],\n",
              " 'sinc': [('doc05', 2.321928094887362)],\n",
              " 'singl': [('doc05', 2.321928094887362)],\n",
              " 'stori': [('doc02', 2.321928094887362)],\n",
              " 'suspens': [('doc04', 2.321928094887362)],\n",
              " 'theater': [('doc05', 2.321928094887362)],\n",
              " 'toy': [('doc02', 2.321928094887362)],\n",
              " 'twist': [('doc04', 2.321928094887362)],\n",
              " 'unexpect': [('doc04', 2.321928094887362)],\n",
              " 'watch': [('doc05', 0.6438561897747247)]}"
            ]
          },
          "execution_count": 463,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def generate_inverted_index(data):\n",
        "\n",
        "    all_words = get_vocabulary(data)  # TODO : obtener el vocabulario\n",
        "    idf_score =  calculate_idf(data)# TODO : calcular idf\n",
        "    tf_idf_score = calculate_tfidf(data,idf_score)  # TODO : calcular if*idf\n",
        "\n",
        "    index = {}\n",
        "    for word in all_words:\n",
        "        for doc, tokens in tf_idf_score.items():\n",
        "            try:            \n",
        "                index[word] = [(doc,tokens[word])]\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "    return index\n",
        "\n",
        "inverted_index = generate_inverted_index (data_docs)\n",
        "\n",
        "inverted_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MvXZOcrFnRh"
      },
      "source": [
        "## 1.8. Resolución de consultas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiVHjcHnkY8h"
      },
      "source": [
        "Vamos a resolver consultas (obtener los documentos más relevantes) considerando la consulta como un vector y comparándolo con el conjunto de documentos mediante la **similitud del coseno**. \n",
        "\n",
        "Para ello vamos a utilizar la librería [sklearn](https://scikit-learn.org/stable/), aunque sólo la funcionalidad para calcular la similitud del coseno. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 464,
      "metadata": {
        "id": "THFDY8QImpsA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alon</th>\n",
              "      <th>anim</th>\n",
              "      <th>best</th>\n",
              "      <th>brother</th>\n",
              "      <th>cold</th>\n",
              "      <th>colleg</th>\n",
              "      <th>dad</th>\n",
              "      <th>ever</th>\n",
              "      <th>fill</th>\n",
              "      <th>film</th>\n",
              "      <th>...</th>\n",
              "      <th>scari</th>\n",
              "      <th>sinc</th>\n",
              "      <th>singl</th>\n",
              "      <th>stori</th>\n",
              "      <th>suspens</th>\n",
              "      <th>theater</th>\n",
              "      <th>toy</th>\n",
              "      <th>twist</th>\n",
              "      <th>unexpect</th>\n",
              "      <th>watch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>q1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    alon  anim  best  brother  cold  colleg  dad  ever  fill  film  ...  \\\n",
              "q1     1     0     0        0     0       0    0     0     0     0  ...   \n",
              "\n",
              "    scari  sinc  singl  stori  suspens  theater  toy  twist  unexpect  watch  \n",
              "q1      0     0      0      0        0        0    0      0         0      1  \n",
              "\n",
              "[1 rows x 33 columns]"
            ]
          },
          "execution_count": 464,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generar un dataframe para la consulta\n",
        "\n",
        "q = \"I watched alone a horror movie\"\n",
        "\n",
        "def generate_query_dataframe ( vocabulary , q ):\n",
        "\n",
        "  # TODO : preprocesar las palabras de la consulta\n",
        "  q_dict = stem_words(remove_stop_words(\n",
        "      tokenize_and_remove_punctuations(q)))\n",
        "  \n",
        "  d =  {}\n",
        "  for i in vocabulary:\n",
        "    if i in q_dict:\n",
        "      d[i] = 1\n",
        "    else:\n",
        "      d[i] = 0 # TODO : generar un dataframe con la misma estructura \n",
        "  table = []  \n",
        "            # TODO : si el token está en la consulta se pone 1 en otro caso 0\n",
        "  table.append (d)\n",
        "\n",
        "  df = pd.DataFrame ( table  , index ={'q1'} )\n",
        "  return df \n",
        "\n",
        "df_query =  generate_query_dataframe ( get_vocabulary(data_docs)  , q )\n",
        "df_query.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "metadata": {
        "id": "CoKt3SbDk4v-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 999 µs\n",
            "[0.09665456 0.03088388 0.50727185 0.02821094 0.11810167]\n",
            "Documento más similar : Watching horror movies alone at night is really scary (doc03) \n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# TODO : calcular la similitud del coseno e indicar qué documento es el más próximo a la consulta\n",
        "%time q_cossim = cosine_similarity(df_data.to_numpy(), df_query.head(1).to_numpy())\n",
        "\n",
        "print (q_cossim.flatten())\n",
        "ind = np.unravel_index ( np.argmax(q_cossim, axis=None), q_cossim.shape)\n",
        "print ('Documento más similar : %s (%s) ' %  (documents[ind[0]],docIds[ind[0]]) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjknLHHHc7KM"
      },
      "source": [
        "# Parte 2. Stack Overflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pe8PIGqfwZq"
      },
      "source": [
        "Stack Overflow es un sitio de preguntas y respuestas para programadores profesionales y aficionados. Contiene preguntas y respuestas sobre una amplia gama de temas de programación.\n",
        "\n",
        "En este ejercicio vamos a usar en subconjunto del dataset original que contiene 500 posts. El dataset se obtiene del volcado de Stack Overflow en Big Query de Google.\n",
        "\n",
        "Lo primero que hay que hacer es leer línea por línea el archivo JSON `stackoverflow-test.json` y meterlo en lun DataFrame de pandas. (Nota: `lines=True` indica que se trate cada línea como una cadena json) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {},
      "outputs": [],
      "source": [
        "stackoverflow_df = pd.read_json(\"stackoverflow-test.json\", lines=True)  # TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 468,
      "metadata": {
        "id": "WsjBy4XshErJ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>accepted_answer_id</th>\n",
              "      <th>answer_count</th>\n",
              "      <th>comment_count</th>\n",
              "      <th>creation_date</th>\n",
              "      <th>last_activity_date</th>\n",
              "      <th>last_edit_date</th>\n",
              "      <th>last_editor_display_name</th>\n",
              "      <th>last_editor_user_id</th>\n",
              "      <th>owner_display_name</th>\n",
              "      <th>owner_user_id</th>\n",
              "      <th>post_type_id</th>\n",
              "      <th>score</th>\n",
              "      <th>tags</th>\n",
              "      <th>view_count</th>\n",
              "      <th>favorite_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3247246</td>\n",
              "      <td>Integrate War-Plugin for m2eclipse into Eclips...</td>\n",
              "      <td>&lt;p&gt;I set up a small web project with JSF and M...</td>\n",
              "      <td>3247526.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2010-07-14 14:39:48.053 UTC</td>\n",
              "      <td>2010-07-14 16:02:19.683 UTC</td>\n",
              "      <td>2010-07-14 15:56:37.803 UTC</td>\n",
              "      <td></td>\n",
              "      <td>70604.0</td>\n",
              "      <td></td>\n",
              "      <td>389430.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>eclipse|maven-2|tomcat|m2eclipse</td>\n",
              "      <td>1653</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40270764</td>\n",
              "      <td>phantomjs-node page.evaulate seems to hang</td>\n",
              "      <td>&lt;p&gt;I have an implementation of 'waitfor' with ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-10-26 19:35:00.537 UTC</td>\n",
              "      <td>2016-11-02 20:05:09.143 UTC</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>245076.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>node.js|phantomjs</td>\n",
              "      <td>35</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27532383</td>\n",
              "      <td>Dynamic operations can only be performed in ho...</td>\n",
              "      <td>&lt;p&gt;I'm working with an API that requires:&lt;/p&gt;\\...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-12-17 18:31:18.6 UTC</td>\n",
              "      <td>2014-12-17 19:57:43.443 UTC</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>3105880.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>c#|asp.net-mvc</td>\n",
              "      <td>4372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33511888</td>\n",
              "      <td>CSS with relative URL to background image?</td>\n",
              "      <td>&lt;p&gt;I have a file structure of:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;co...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2015-11-04 00:50:35.223 UTC</td>\n",
              "      <td>2015-11-04 01:51:03.037 UTC</td>\n",
              "      <td>2015-11-04 01:51:03.037 UTC</td>\n",
              "      <td></td>\n",
              "      <td>5464492.0</td>\n",
              "      <td></td>\n",
              "      <td>5464492.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>css|background-image</td>\n",
              "      <td>406</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>46160163</td>\n",
              "      <td>Share canvas image on android</td>\n",
              "      <td>&lt;p&gt;Hello so I write a small game where in the ...</td>\n",
              "      <td>46160246.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-09-11 16:19:18.32 UTC</td>\n",
              "      <td>2017-09-11 16:24:12.69 UTC</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>8570512.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>android|canvas|bitmap|share</td>\n",
              "      <td>52</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                              title  \\\n",
              "0   3247246  Integrate War-Plugin for m2eclipse into Eclips...   \n",
              "1  40270764         phantomjs-node page.evaulate seems to hang   \n",
              "2  27532383  Dynamic operations can only be performed in ho...   \n",
              "3  33511888         CSS with relative URL to background image?   \n",
              "4  46160163                      Share canvas image on android   \n",
              "\n",
              "                                                body  accepted_answer_id  \\\n",
              "0  <p>I set up a small web project with JSF and M...           3247526.0   \n",
              "1  <p>I have an implementation of 'waitfor' with ...                 NaN   \n",
              "2  <p>I'm working with an API that requires:</p>\\...                 NaN   \n",
              "3  <p>I have a file structure of:</p>\\n\\n<pre><co...                 NaN   \n",
              "4  <p>Hello so I write a small game where in the ...          46160246.0   \n",
              "\n",
              "   answer_count  comment_count                creation_date  \\\n",
              "0             2              0  2010-07-14 14:39:48.053 UTC   \n",
              "1             1              0  2016-10-26 19:35:00.537 UTC   \n",
              "2             1              0    2014-12-17 18:31:18.6 UTC   \n",
              "3             2              2  2015-11-04 00:50:35.223 UTC   \n",
              "4             1              0   2017-09-11 16:19:18.32 UTC   \n",
              "\n",
              "            last_activity_date               last_edit_date  \\\n",
              "0  2010-07-14 16:02:19.683 UTC  2010-07-14 15:56:37.803 UTC   \n",
              "1  2016-11-02 20:05:09.143 UTC                          NaN   \n",
              "2  2014-12-17 19:57:43.443 UTC                          NaN   \n",
              "3  2015-11-04 01:51:03.037 UTC  2015-11-04 01:51:03.037 UTC   \n",
              "4   2017-09-11 16:24:12.69 UTC                          NaN   \n",
              "\n",
              "  last_editor_display_name  last_editor_user_id owner_display_name  \\\n",
              "0                                       70604.0                      \n",
              "1                                           NaN                      \n",
              "2                                           NaN                      \n",
              "3                                     5464492.0                      \n",
              "4                                           NaN                      \n",
              "\n",
              "   owner_user_id  post_type_id  score                              tags  \\\n",
              "0       389430.0             1      2  eclipse|maven-2|tomcat|m2eclipse   \n",
              "1       245076.0             1      0                 node.js|phantomjs   \n",
              "2      3105880.0             1      1                    c#|asp.net-mvc   \n",
              "3      5464492.0             1      0              css|background-image   \n",
              "4      8570512.0             1      0       android|canvas|bitmap|share   \n",
              "\n",
              "   view_count  favorite_count  \n",
              "0        1653             NaN  \n",
              "1          35             NaN  \n",
              "2        4372             NaN  \n",
              "3         406             NaN  \n",
              "4          52             NaN  "
            ]
          },
          "execution_count": 468,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stackoverflow_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WOnJi3lmMD7"
      },
      "source": [
        "Examinar el dataset para ver qué dimensiones y qué tipos de datos contiene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 469,
      "metadata": {
        "id": "thpXzyqZh3eM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 18 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   id                        500 non-null    int64  \n",
            " 1   title                     500 non-null    object \n",
            " 2   body                      500 non-null    object \n",
            " 3   accepted_answer_id        253 non-null    float64\n",
            " 4   answer_count              500 non-null    int64  \n",
            " 5   comment_count             500 non-null    int64  \n",
            " 6   creation_date             500 non-null    object \n",
            " 7   last_activity_date        500 non-null    object \n",
            " 8   last_edit_date            257 non-null    object \n",
            " 9   last_editor_display_name  500 non-null    object \n",
            " 10  last_editor_user_id       254 non-null    float64\n",
            " 11  owner_display_name        500 non-null    object \n",
            " 12  owner_user_id             494 non-null    float64\n",
            " 13  post_type_id              500 non-null    int64  \n",
            " 14  score                     500 non-null    int64  \n",
            " 15  tags                      500 non-null    object \n",
            " 16  view_count                500 non-null    int64  \n",
            " 17  favorite_count            87 non-null     float64\n",
            "dtypes: float64(4), int64(6), object(8)\n",
            "memory usage: 70.4+ KB\n"
          ]
        }
      ],
      "source": [
        "stackoverflow_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeqQnrOdrJ5x"
      },
      "source": [
        "Observad que el dataset tiene 19 campos que incluyen título `title`, cuerpo `body`, `tags`, fechas y otros metadatos que no necesitaremos en esta ocasión. \n",
        "\n",
        "\n",
        "El interés se centra en el cuerpo y en el título que forman la fuente de texto.Para ello vamos a crear un campo `text` que combina `title` y `body` en uno solo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 470,
      "metadata": {
        "id": "DQPzgn7hiQSC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"CSS with relative URL to background image?<p>I have a file structure of:</p>\\n\\n<pre><code>home.html\\nimg/bg_damask1.jpg\\ncss/style.css\\n</code></pre>\\n\\n<p>When I set my body background image I can't get it to load. I've tried:</p>\\n\\n<pre><code>background-image: url('../img/bg_damask1.jpg');\\nbackground-image: url('/img/bg_damask1.jpg');\\nbackground-image: url('img/bg_damask1.jpg');\\n</code></pre>\\n\\n<p>But none are working. How do I get my css to reference the background image?</p>\\n\\n<p>ETA: In browser dev tools I see that no matter what file path I put in, the browser is only referencing 'bg_damask1.jpg' without the file path. If I edit it in dev tools the image shows up using option #1. Now I'm stumped as to what's causing the breakdown.</p>\""
            ]
          },
          "execution_count": 470,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stackoverflow_df['text'] = stackoverflow_df['title'].astype(\n",
        "    str) + stackoverflow_df['body'].astype(str)  # TODO\n",
        "stackoverflow_df['text'][3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {},
      "outputs": [],
      "source": [
        "stackoverflow_df['mtext']= stackoverflow_df['text'].apply(preprocess_text,stem=False)\n",
        "stackoverflow_df['mtext']=stackoverflow_df['mtext'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZuayg_UhYia"
      },
      "source": [
        "Como se puede observar el texto contiene bastante \"ruido\" y es necesario procesarlo para que pueda ser útil. Hay que conseguir una cadena de texto en el que se eliminen palabras vacías, tags de html, carácteres extraños, etc. No resulta recomendable lematizar las palabras en esta ocasión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {
        "id": "eISZT4RMBnXm"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>accepted_answer_id</th>\n",
              "      <th>answer_count</th>\n",
              "      <th>comment_count</th>\n",
              "      <th>creation_date</th>\n",
              "      <th>last_activity_date</th>\n",
              "      <th>last_edit_date</th>\n",
              "      <th>last_editor_display_name</th>\n",
              "      <th>last_editor_user_id</th>\n",
              "      <th>owner_display_name</th>\n",
              "      <th>owner_user_id</th>\n",
              "      <th>post_type_id</th>\n",
              "      <th>score</th>\n",
              "      <th>tags</th>\n",
              "      <th>view_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>text</th>\n",
              "      <th>mtext</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3247246</td>\n",
              "      <td>Integrate War-Plugin for m2eclipse into Eclips...</td>\n",
              "      <td>&lt;p&gt;I set up a small web project with JSF and M...</td>\n",
              "      <td>3247526.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2010-07-14 14:39:48.053 UTC</td>\n",
              "      <td>2010-07-14 16:02:19.683 UTC</td>\n",
              "      <td>2010-07-14 15:56:37.803 UTC</td>\n",
              "      <td></td>\n",
              "      <td>70604.0</td>\n",
              "      <td></td>\n",
              "      <td>389430.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>eclipse|maven-2|tomcat|m2eclipse</td>\n",
              "      <td>1653</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Integrate War-Plugin for m2eclipse into Eclips...</td>\n",
              "      <td>['integrate', 'war', 'plugin', 'meclipse', 'ec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40270764</td>\n",
              "      <td>phantomjs-node page.evaulate seems to hang</td>\n",
              "      <td>&lt;p&gt;I have an implementation of 'waitfor' with ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-10-26 19:35:00.537 UTC</td>\n",
              "      <td>2016-11-02 20:05:09.143 UTC</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>245076.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>node.js|phantomjs</td>\n",
              "      <td>35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>phantomjs-node page.evaulate seems to hang&lt;p&gt;I...</td>\n",
              "      <td>['phantomjs', 'node', 'pageevaulate', 'seems',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27532383</td>\n",
              "      <td>Dynamic operations can only be performed in ho...</td>\n",
              "      <td>&lt;p&gt;I'm working with an API that requires:&lt;/p&gt;\\...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-12-17 18:31:18.6 UTC</td>\n",
              "      <td>2014-12-17 19:57:43.443 UTC</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>3105880.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>c#|asp.net-mvc</td>\n",
              "      <td>4372</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Dynamic operations can only be performed in ho...</td>\n",
              "      <td>['dynamic', 'operations', 'performed', 'homoge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33511888</td>\n",
              "      <td>CSS with relative URL to background image?</td>\n",
              "      <td>&lt;p&gt;I have a file structure of:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;co...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2015-11-04 00:50:35.223 UTC</td>\n",
              "      <td>2015-11-04 01:51:03.037 UTC</td>\n",
              "      <td>2015-11-04 01:51:03.037 UTC</td>\n",
              "      <td></td>\n",
              "      <td>5464492.0</td>\n",
              "      <td></td>\n",
              "      <td>5464492.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>css|background-image</td>\n",
              "      <td>406</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CSS with relative URL to background image?&lt;p&gt;I...</td>\n",
              "      <td>['css', 'relative', 'url', 'background', 'imag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>46160163</td>\n",
              "      <td>Share canvas image on android</td>\n",
              "      <td>&lt;p&gt;Hello so I write a small game where in the ...</td>\n",
              "      <td>46160246.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-09-11 16:19:18.32 UTC</td>\n",
              "      <td>2017-09-11 16:24:12.69 UTC</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>8570512.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>android|canvas|bitmap|share</td>\n",
              "      <td>52</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Share canvas image on android&lt;p&gt;Hello so I wri...</td>\n",
              "      <td>['share', 'canvas', 'image', 'androidhello', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                              title  \\\n",
              "0   3247246  Integrate War-Plugin for m2eclipse into Eclips...   \n",
              "1  40270764         phantomjs-node page.evaulate seems to hang   \n",
              "2  27532383  Dynamic operations can only be performed in ho...   \n",
              "3  33511888         CSS with relative URL to background image?   \n",
              "4  46160163                      Share canvas image on android   \n",
              "\n",
              "                                                body  accepted_answer_id  \\\n",
              "0  <p>I set up a small web project with JSF and M...           3247526.0   \n",
              "1  <p>I have an implementation of 'waitfor' with ...                 NaN   \n",
              "2  <p>I'm working with an API that requires:</p>\\...                 NaN   \n",
              "3  <p>I have a file structure of:</p>\\n\\n<pre><co...                 NaN   \n",
              "4  <p>Hello so I write a small game where in the ...          46160246.0   \n",
              "\n",
              "   answer_count  comment_count                creation_date  \\\n",
              "0             2              0  2010-07-14 14:39:48.053 UTC   \n",
              "1             1              0  2016-10-26 19:35:00.537 UTC   \n",
              "2             1              0    2014-12-17 18:31:18.6 UTC   \n",
              "3             2              2  2015-11-04 00:50:35.223 UTC   \n",
              "4             1              0   2017-09-11 16:19:18.32 UTC   \n",
              "\n",
              "            last_activity_date               last_edit_date  \\\n",
              "0  2010-07-14 16:02:19.683 UTC  2010-07-14 15:56:37.803 UTC   \n",
              "1  2016-11-02 20:05:09.143 UTC                          NaN   \n",
              "2  2014-12-17 19:57:43.443 UTC                          NaN   \n",
              "3  2015-11-04 01:51:03.037 UTC  2015-11-04 01:51:03.037 UTC   \n",
              "4   2017-09-11 16:24:12.69 UTC                          NaN   \n",
              "\n",
              "  last_editor_display_name  last_editor_user_id owner_display_name  \\\n",
              "0                                       70604.0                      \n",
              "1                                           NaN                      \n",
              "2                                           NaN                      \n",
              "3                                     5464492.0                      \n",
              "4                                           NaN                      \n",
              "\n",
              "   owner_user_id  post_type_id  score                              tags  \\\n",
              "0       389430.0             1      2  eclipse|maven-2|tomcat|m2eclipse   \n",
              "1       245076.0             1      0                 node.js|phantomjs   \n",
              "2      3105880.0             1      1                    c#|asp.net-mvc   \n",
              "3      5464492.0             1      0              css|background-image   \n",
              "4      8570512.0             1      0       android|canvas|bitmap|share   \n",
              "\n",
              "   view_count  favorite_count  \\\n",
              "0        1653             NaN   \n",
              "1          35             NaN   \n",
              "2        4372             NaN   \n",
              "3         406             NaN   \n",
              "4          52             NaN   \n",
              "\n",
              "                                                text  \\\n",
              "0  Integrate War-Plugin for m2eclipse into Eclips...   \n",
              "1  phantomjs-node page.evaulate seems to hang<p>I...   \n",
              "2  Dynamic operations can only be performed in ho...   \n",
              "3  CSS with relative URL to background image?<p>I...   \n",
              "4  Share canvas image on android<p>Hello so I wri...   \n",
              "\n",
              "                                               mtext  \n",
              "0  ['integrate', 'war', 'plugin', 'meclipse', 'ec...  \n",
              "1  ['phantomjs', 'node', 'pageevaulate', 'seems',...  \n",
              "2  ['dynamic', 'operations', 'performed', 'homoge...  \n",
              "3  ['css', 'relative', 'url', 'background', 'imag...  \n",
              "4  ['share', 'canvas', 'image', 'androidhello', '...  "
            ]
          },
          "execution_count": 472,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stackoverflow_df.head(5)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8MHVA4fTXvv"
      },
      "source": [
        "## 2.1 Count Vectorizer\n",
        "\n",
        "A continuación se necesita obtener un vocabulario de palabras usadas y comenzar el proceso de contabilización.\n",
        "\n",
        "La **vectorización** es el proceso por el que se convierte una colección de textos en un vector de características numérico. El modelo que seguimos es el de bolsa de palabras o Bag-of-Words, donde los documentos se describen por las palabras que aparecen en el texto, ignorando su posición relativa o su importancia en el texto.\n",
        "\n",
        "**CountVectorizer** convierte una colección de documentos en una matriz de contadores que son las apariciones de cada token en cada documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {
        "id": "dDjtJoe3DTu_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer (max_df=0.85) # max_df : eliminar si la palabra está en el 85% de los documentos\n",
        "\n",
        "word_count_vector = vectorizer.fit_transform(stackoverflow_df['mtext'])# TODO : transformar el texto limpio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500, 10239)"
            ]
          },
          "execution_count": 484,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_count_vector.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnllSkXo68Pe"
      },
      "source": [
        "Una vez contabilizado el texto se pueden ver algunas palabras del vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "metadata": {
        "id": "a2ipp9nrELRe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['integrate',\n",
              " 'war',\n",
              " 'plugin',\n",
              " 'meclipse',\n",
              " 'eclipse',\n",
              " 'projecti',\n",
              " 'set',\n",
              " 'small',\n",
              " 'web',\n",
              " 'project']"
            ]
          },
          "execution_count": 474,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(vectorizer.vocabulary_.keys())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnlFNh8HjLRC"
      },
      "source": [
        "Las columnas de la matriz son las características o tokens detectados en el texto y se pueden ver con la función `get_feature_names()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {
        "id": "yPQu5OmX6kzC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['zlibinflatedtostring', 'zlibinflatesync', 'zmainid', 'zoom', 'zoomed', 'zstreams', 'zutilitytypename', 'zygoteinitjava', 'δeq', 'ﬁnal']\n"
          ]
        }
      ],
      "source": [
        "print (vectorizer.get_feature_names()[-10:] )\n",
        "\n",
        "vocabulary = vectorizer.get_feature_names()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTxCF6_97Q_6"
      },
      "source": [
        "El resultado es una matriz `sparse matrix` que representa las cuentas de las palabras. Cada columna representa una palabra en el vocabulario y cada fila representa un documento (un post) en el datataset y cada celda el números de apariciones de la palabra en el documento.\n",
        "\n",
        "**¿Cuál es el tamaño del vocabulario?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aapt\n"
          ]
        }
      ],
      "source": [
        "print(vocabulary[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wc_df = pd.DataFrame(word_count_vector.toarray(), columns=vectorizer.get_feature_names(\n",
        "# ), index=[f'document[{x}]' for x in range(0, word_count_vector.shape[0])])  # TODO\n",
        "# wc_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw3GweyA9gfZ"
      },
      "source": [
        "Por comodidad, podemos convertir la matriz en un DataFrame en el que las filas sean los documentos y las columnas el vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {
        "id": "OpquSy5U-pqx"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aapl</th>\n",
              "      <th>aapt</th>\n",
              "      <th>abandoned</th>\n",
              "      <th>abbrev</th>\n",
              "      <th>abbrevtostring</th>\n",
              "      <th>abby</th>\n",
              "      <th>abbytransformposition</th>\n",
              "      <th>abbyvelocityf</th>\n",
              "      <th>abc</th>\n",
              "      <th>abcjpg</th>\n",
              "      <th>...</th>\n",
              "      <th>zlibinflatedtostring</th>\n",
              "      <th>zlibinflatesync</th>\n",
              "      <th>zmainid</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zoomed</th>\n",
              "      <th>zstreams</th>\n",
              "      <th>zutilitytypename</th>\n",
              "      <th>zygoteinitjava</th>\n",
              "      <th>δeq</th>\n",
              "      <th>ﬁnal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 10239 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aapl  aapt  abandoned  abbrev  abbrevtostring  abby  abbytransformposition  \\\n",
              "0     0     0          0       0               0     0                      0   \n",
              "1     0     0          0       0               0     0                      0   \n",
              "2     0     0          0       0               0     0                      0   \n",
              "3     0     0          0       0               0     0                      0   \n",
              "4     0     0          0       0               0     0                      0   \n",
              "5     0     0          0       0               0     0                      0   \n",
              "6     0     0          0       0               0     0                      0   \n",
              "7     0     0          0       0               0     0                      0   \n",
              "8     0     0          0       0               0     0                      0   \n",
              "9     0     0          0       0               0     0                      0   \n",
              "\n",
              "   abbyvelocityf  abc  abcjpg  ...  zlibinflatedtostring  zlibinflatesync  \\\n",
              "0              0    0       0  ...                     0                0   \n",
              "1              0    0       0  ...                     0                0   \n",
              "2              0    0       0  ...                     0                0   \n",
              "3              0    0       0  ...                     0                0   \n",
              "4              0    0       0  ...                     0                0   \n",
              "5              0    0       0  ...                     0                0   \n",
              "6              0    0       0  ...                     0                0   \n",
              "7              0    0       0  ...                     0                0   \n",
              "8              0    0       0  ...                     0                0   \n",
              "9              0    0       0  ...                     0                0   \n",
              "\n",
              "   zmainid  zoom  zoomed  zstreams  zutilitytypename  zygoteinitjava  δeq  \\\n",
              "0        0     0       0         0                 0               0    0   \n",
              "1        0     0       0         0                 0               0    0   \n",
              "2        0     0       0         0                 0               0    0   \n",
              "3        0     0       0         0                 0               0    0   \n",
              "4        0     0       0         0                 0               0    0   \n",
              "5        0     0       0         0                 0               0    0   \n",
              "6        0     0       0         0                 0               0    0   \n",
              "7        0     0       0         0                 0               0    0   \n",
              "8        0     0       0         0                 0               0    0   \n",
              "9        0     0       0         0                 0               0    0   \n",
              "\n",
              "   ﬁnal  \n",
              "0     0  \n",
              "1     0  \n",
              "2     0  \n",
              "3     0  \n",
              "4     0  \n",
              "5     0  \n",
              "6     0  \n",
              "7     0  \n",
              "8     0  \n",
              "9     0  \n",
              "\n",
              "[10 rows x 10239 columns]"
            ]
          },
          "execution_count": 478,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wc_df = pd.DataFrame(word_count_vector.toarray(), columns=vectorizer.get_feature_names(\n",
        "))  # TODO\n",
        "wc_df.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyrNMavD-Y_p"
      },
      "source": [
        "Con del DataFrame es más fácil hacer consultas. \n",
        "\n",
        "**¿En qué post de stackoverflow aparece más veces la palabra `eclipse`?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "metadata": {
        "id": "95L2ivubDRjh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "300    ['running', 'executable', 'jar', 'file', 'usin...\n",
              "Name: mtext, dtype: object"
            ]
          },
          "execution_count": 480,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eclipse_idx = wc_df[[\"eclipse\"]].idxmax()[0]  # TODO\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 50)\n",
        "stackoverflow_df[stackoverflow_df.index == eclipse_idx]['mtext']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGg98DxE4I1r"
      },
      "source": [
        "## 2.2 TF*IDF TfidfTransformer\n",
        "\n",
        "\n",
        "Ya hemos visto en qué consiste el método $TF*IDF$. Con el objeto **TfidfTransformer** se pueden calcular las puntuaciones TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 482,
      "metadata": {
        "id": "eCGm_YVOQrGJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer (smooth_idf=True, use_idf=True)\n",
        "tf_idf_matrix = transformer.fit_transform(word_count_vector)  #TODO : transformar word_count_vector en tf*idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByyR06R1Fh0e"
      },
      "source": [
        "Este código ha generado una matriz de puntuaciones tf-idf. ¿Las dimensiones de esta matriz coinciden con la obtenida de `CountVectorize`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 483,
      "metadata": {
        "id": "xvhkjYkeD3Nu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500, 10239)"
            ]
          },
          "execution_count": 483,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_idf_matrix.shape #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ykVK8vGxkv"
      },
      "source": [
        "De la misma forma, vamos a convertir la matriz en un DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpwfeDaNSDuQ"
      },
      "outputs": [],
      "source": [
        "tf_idf_df = pd.DataFrame () # TODO\n",
        "\n",
        "tf_idf_df[tf_idf_df['python']>0.1]['python']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luQVcQwXRkfq"
      },
      "source": [
        "Es posible ordenar valores tf-idf en orden descendente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSxIFojRj_i"
      },
      "outputs": [],
      "source": [
        "tf_idf_df['python'].nlargest(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6uSHRLUKcZD"
      },
      "source": [
        "El objetivo a continuación es extraer las $n$ palabras clave (`keywords`) que mejor representen cada una de las cuestiones planteadas en los datos de stackoverflow, según las puntuaciones tf-idf.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwAtG_xQMlVV"
      },
      "outputs": [],
      "source": [
        "def extract_topn_keywords (row, vocabulary, topn=5):\n",
        "  # TODO \n",
        "  # TODO\n",
        "  \n",
        "  result = {}\n",
        "  # TODO\n",
        "   \n",
        "  return result\n",
        "  \n",
        "for row in tf_idf_df[0:2].iterrows():\n",
        "  print (extract_topn_keywords(row[1] , vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TWrB6ZBI8AO"
      },
      "outputs": [],
      "source": [
        "stackoverflow_df['keywords'] =  #TODO : para cada documento generar una columna con el diccionario obtenido\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-gALLOxKVTL"
      },
      "outputs": [],
      "source": [
        "stackoverflow_df.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z815_bdDIPmT"
      },
      "source": [
        "Ahora hay que buscar los dos posts que sean más similares entre sí. Para ello vamos a calcular la **similitud del coseno** entre las matrices tf- idf:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiDPGp7SGqBd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "tf_idf_cossim =  # TODO\n",
        "np.fill_diagonal(tf_idf_cossim, 0.0) # anular la diagonal principal\n",
        "ind =  # TODO: localizar el máximo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM7oASAeRjUy"
      },
      "outputs": [],
      "source": [
        "# TODO : visualizar ambos posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLV-wDXgcgAl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "f, ax = plt.subplots(figsize=(15, 15))\n",
        "\n",
        "A = tf_idf_cossim.flatten().reshape (500,500)\n",
        "\n",
        "mask = np.zeros_like(A, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "ax = sns.heatmap(A, mask=mask, square=True,  fmt='.2f', cbar_kws={\"shrink\": .5},\n",
        "                 vmax=1.0, vmin=.3, cmap=\"hot\")\n",
        "\n",
        "ax.set_title(\"Heatmap of cosine similarity scores\").set_fontsize(15)\n",
        "\n",
        "\n",
        "ax.set_xlabel(\"\")\n",
        "ax.set_ylabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZfbHaqTH_-4"
      },
      "source": [
        "En esta última sección,  vamos a presentar el algoritmo de clustering [K-Means](https://en.wikipedia.org/wiki/K-means_clustering), un algoritmo de clustering no supervisado que nos va a agrupar las diferentes cuestiones planteadas en Stack Overflow en diferentes grupos.\n",
        "\n",
        "\n",
        "El objetivo de K-Means es sencillo: agrupar datos similares y descubrir patrones subyacentes. Para lograr esto, K-Means busca un número fijo $k$ de centroides en el dataset. Un centroide es el centro de un cluster y un cluster es una collección de datos reunidos que tienen ciertas similitudes entre ellos. El ‘means’ se refiere al promediado de los datos encontrando el centroide. El algoritmo es no supervisado pues no tiene conocimiento de los grupos en el dataset, es decir, encontrará los grupos subyacentes en el dataset.\n",
        "\n",
        "Vamos a ver cómo funciona K-Means para los datos de tf-idf:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HV3GceGH-2B"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 5 \n",
        "kmeans = KMeans(n_clusters=k).fit(tf_idf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt_G930ypJN5"
      },
      "source": [
        "El algoritmo ha agrupado cada uno de los documentos en uno de los 5 cluster definidos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chxlhLdmI-hq"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zFtfv5lWKr"
      },
      "source": [
        "Una cuestión importante es que el parámetro que indica el número de cluster $k$ hay que proporcionárselo y no sabemos, a priori, cuántos clúster podría haber subyacentes.\n",
        "\n",
        "Para ello vamos a realizar el proceso con varios valores de $k$ para ver cuál es más adecuado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1QQ_BbtwV3Z"
      },
      "outputs": [],
      "source": [
        "def run_KMeans(max_k, data):\n",
        "    max_k += 1\n",
        "    kmeans_results = dict()\n",
        "    for k in range(2 , max_k):\n",
        "        kmeans = KMeans(n_clusters =  # TODO\n",
        "                        , init = 'k-means++'\n",
        "                        , n_init = 10\n",
        "                        , tol = 0.0001\n",
        "                        , n_jobs = -1\n",
        "                        , random_state = 1\n",
        "                        , algorithm = 'full')\n",
        "\n",
        "        kmeans_results [] =  # TODO : poner en un diccionario\n",
        "    return kmeans_results\n",
        "\n",
        "kmeans_results = kmeans_results = run_KMeans (12, tf_idf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnCTB3EOxxAk"
      },
      "source": [
        "Para determinar cómo de cohesionado está un cluster en relación con los otros clusters existe una medida denominada silueta (silhouette) que nos indica precisamente esto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTQXwHlcxreD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def silhouette(kmeans_dict, df, plot=False):\n",
        "    sil = dict()\n",
        "    for k, kmeans in kmeans_dict.items():      \n",
        "        kmeans_labels = kmeans.predict(df)\n",
        "        silhouette_avg = silhouette_score(df, kmeans_labels) \n",
        "        sil[] = # TODO: añadir a diccionario\n",
        "\n",
        "    return sil\n",
        "\n",
        "sil_results = silhouette ( kmeans_results, tf_idf_df )\n",
        "print (sil_results)\n",
        "\n",
        "# TODO: Qué valor de k obtiene el mejor valor de silueta \n",
        "best_k = \n",
        "print (best_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItH9V4Aj1RxN"
      },
      "source": [
        "Una vez obtenido el valor de cluster que mejor podría describir la agrupación de los datos, vamos a hacer una predicción sobre los datos con la función `fit_predict` con el objeto kmeans que corresponde a `best_k`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMgsTiRs1qnJ"
      },
      "outputs": [],
      "source": [
        "prediction =  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgrMybQ64wll"
      },
      "outputs": [],
      "source": [
        "stackoverflow_df['cluster'] =  # TODO: añadir las predicciones a stackoverflow_df\n",
        "\n",
        "stackoverflow_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG9EbGQS3NPm"
      },
      "source": [
        "Una vez definidos los clusters, vamos a ver cuáles son las palabras clave que definen cada uno de los clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtgtDfIbMBfX"
      },
      "outputs": [],
      "source": [
        "# Definir un par de funciones auxiliares\n",
        "\n",
        "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
        "    labels = np.unique(prediction)\n",
        "    dfs = []\n",
        "    for label in labels:\n",
        "        id_temp = np.where(prediction==label) # \n",
        "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # \n",
        "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # \n",
        "        features = vectorizer.get_feature_names()\n",
        "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
        "        df = pd.DataFrame(best_features, columns = ['words', 'score'])\n",
        "        dfs.append(df)\n",
        "    return dfs\n",
        "\n",
        "\n",
        "def plotWords(dfs, n_feats):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    for i in range(0, len(dfs)):\n",
        "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
        "        sns.barplot(x = 'score' , y = 'words', orient = 'h' , data = dfs[i][:n_feats])\n",
        "        plt.show()\n",
        "\n",
        "n_feats = 5 # Num. palabras a representar\n",
        "dfs = get_top_features_cluster(tf_idf_df.to_numpy(), prediction, n_feats)\n",
        "plotWords(dfs, n_feats)  # "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JQ9W5Cd4BRK"
      },
      "source": [
        "**¿En qué cluster se encuentran las preguntas relativas a Android?** \n",
        "\n",
        "**¿De qué tratan los post/preguntas/documentos que se encuentran en el cluster 7?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJs5pMB6Z0xx"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw6yDxRZL7j9"
      },
      "source": [
        "# Parte 3. Declaración Universal de los Derechos Humanos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vcePD0XLZGb"
      },
      "source": [
        "La [Declaración Universal de los Derechos Humanos](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) es un documento adoptado por la Asamblea General de las Naciones Unidas en su Resolución 217 A (III), el 10 de diciembre de 1948 en París, ​ que recoge en sus 30 artículos los derechos humanos considerados básicos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltAarFcOFTdq"
      },
      "outputs": [],
      "source": [
        "nltk.download('udhr')\n",
        "  \n",
        "from nltk.corpus import udhr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyvGzUKHU-fe"
      },
      "source": [
        "Una vez importado el contenido, para ver los idiomas en los que se encuentran disponible:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPRGkwyIFTgP"
      },
      "outputs": [],
      "source": [
        "udhr.fileids()[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lleMQNcLWVmk"
      },
      "source": [
        "Vamos a seleccionar 5 idiomas para recopilar estadísticas: 4 vienen prefijados y 1 será de tu elección"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwZx3gfUWksI"
      },
      "outputs": [],
      "source": [
        "langs = ['English-Latin1', 'Spanish_Espanol-Latin1', 'French_Francais-Latin1',\n",
        "         'Catalan_Catala-Latin1', ''] #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuyYMOl5Gjm3"
      },
      "source": [
        "Crear un Dataframe con los siguientes datos de cada idioma:\n",
        "\n",
        "- Número de palabras en la Declaración Universal de Derecho Humanos (UDHR, en inglés)\n",
        "\n",
        "- Número de palabras únicas\n",
        "\n",
        "- Longitud media de las palabras\n",
        "\n",
        "- Número de sentencias incluidas\n",
        "\n",
        "- Número medio de palabras por sentencia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX8xIWuvYke9"
      },
      "outputs": [],
      "source": [
        "def generate_udhr_stats():\n",
        "  np_udhr=[]\n",
        "  columns=['Language', 'WordCount','WordCountUnique',\n",
        "           'NumberSentences','MeanWordLength','MeanWordsPerSentence']\n",
        "          \n",
        "\n",
        "    for lang in langs:\n",
        "    text = udhr.raw (lang)\n",
        "    # TODO\n",
        "    # TODO\n",
        "    # TODO\n",
        "    # TODO\n",
        "    # TODO\n",
        "    # TODO\n",
        "    \n",
        "    np_udhr.       # TODO\n",
        "  return pd.DataFrame() # TODO\n",
        "\n",
        "udhr_df = generate_udhr_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvxuxWbCYlSu"
      },
      "outputs": [],
      "source": [
        "udhr_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vycf58eggFo7"
      },
      "source": [
        "¿Cuál es la ***diversidad léxica*** (relación del número de palabras distintas entre el número total de palabras) en cada idioma? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rueuAmT5YlC0"
      },
      "outputs": [],
      "source": [
        "def lexical_diversity (row):\n",
        "  return  # TODO\n",
        "\n",
        "udhr_df['LD'] =  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD9ZuH1kbjSE"
      },
      "outputs": [],
      "source": [
        "udhr_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko3HB4odxdT"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2FNmKC34Ny8"
      },
      "source": [
        "\n",
        "La **ley Zipf** debe su nombre al lingüista norteamericano George Kingsley Zipf y dictamina que un pequeño número de palabras se utilizan todo el tiempo mientras que una gran mayoría de ellas apenas se utiliza. No es muy sorprendente que palabras muy frecuentes en textos en inglés sean `the`, `of` y similares, y que palabras como `intrauterine` apenas se utilicen. [https://en.wikipedia.org/wiki/Zipf%27s_law]\n",
        "\n",
        "La ley Zipf puede escribirse de la siguiente forma: la $r$-ésima palabra más frecuente $f(r)$ escala según la fórmula: $f(r) \\propto \\frac{1}{r^{\\alpha}}$ con $\\alpha \\approx 1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGyoBDzhjb1K"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text = udhr.raw( ) # TODO : elegir un idioma \n",
        "\n",
        "token_dict =  # TODO : obtener la frecuencia de todas las palabras \n",
        "sorted_token_dict =  # TODO : ordenar y quedarse con las 50 más frecuentes\n",
        "\n",
        "\n",
        "sorted_token_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiwBZ6jctqSp"
      },
      "outputs": [],
      "source": [
        "def createZipfTable(freqs):\n",
        "  zipf_table = []\n",
        "  top_frequency =   # TODO : la palabra más frecuente\n",
        "\n",
        "  for index, item in enumerate(freqs , start=1):\n",
        "    relative_frequency = \"1/{}\".format(index)\n",
        "    zipf_frequency = top_frequency * (1 / index)\n",
        "    zipf_table.append({\"word\": item[0], \"actual_frequency\": item[1], \n",
        "                       \"relative_frequency\": relative_frequency , \n",
        "                       \"zipf_frequency\": zipf_frequency })\n",
        "\n",
        "  return zipf_table\n",
        "\n",
        "\n",
        "zipf_table = createZipfTable (sorted_token_list)\n",
        "\n",
        "\n",
        "print(\"|Rank|    Word    |       Freq | Zipf Frac  | Zipf Freq  |\")\n",
        "format_string = \"|{:4}|{:12}|{:12.0f}|{:>12}|{:12.2f}|\"\n",
        "for index, item in enumerate(zipf_table,start=1):\n",
        "        print(format_string.format(index,item[\"word\"],item[\"actual_frequency\"],\n",
        "                                   item[\"relative_frequency\"],item[\"zipf_frequency\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36L1eBHuvE_y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ranks = list (range ( 1, 1+len (zipf_table) ))\n",
        "frequencies = [ rec['actual_frequency']  for rec in zipf_table ]\n",
        "zipf_frequencies = [ rec['zipf_frequency'] for rec in zipf_table ] \n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "plt.title(\"Word Frequencies in UDHR:\")\n",
        "plt.ylabel(\"Total Number of Occurrences\")\n",
        "plt.xlabel(\"Rank of words\")\n",
        "\n",
        "plt.plot(\n",
        "    ranks,\n",
        "    frequencies, color='blue', label='Actual freq.',\n",
        "    alpha=0.5\n",
        "  )\n",
        "\n",
        "\n",
        "plt.plot(\n",
        "    ranks,\n",
        "    zipf_frequencies, color=\"orange\", label='Expected Zipf.',linestyle='--',\n",
        "    alpha=0.5\n",
        "  )\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI38AEP1oShg"
      },
      "source": [
        "**Comparar si los diferentes idiomas elegidos siguen la ley Zipf o divergen de ella.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TyFdD-PEC2-2021.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d68aa252cefc0d28dc6400d41d4c9673702e640c5ae66b886190fff64ef72be0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit (windows store)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
